{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network methods\n",
    "\n",
    "Author: Gaurav Vaidya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning objectives\n",
    "* Understand what a neural network is and how it can be used.\n",
    "* Implement a neural network to label data based on multiple input features.\n",
    "* Understand when a neural network might be useful and when it might be worse than other methods.\n",
    "* Learn where to learn more about neural networks.\n",
    "\n",
    "## Learn deeply\n",
    "[Artificial Neural Networks (ANNs)](https://en.wikipedia.org/wiki/Artificial_neural_network) and [deep learning](https://en.wikipedia.org/wiki/Deep_learning) are currently getting a lot of interest, both as a subject of research and as a tool for analyzing datasets. ANNs are similar to feature crosses, except that thanks to something called [backpropagation](https://en.wikipedia.org/wiki/Backpropagation), an ANN is able to choose its own features based on the testing data. A lot of the other advantages of ANNs are related specifically to interpreting visual and auditory data, which we won't be doing today, but I'll point you some resources to learn about [convolutional neural networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) yourself.\n",
    "\n",
    "## Reminders of the ground rules\n",
    "* Always have training data (used to train the ANN) and testing data (used to test how well the ANN might work against data it has never seen before).\n",
    "* Never, ever, ever, ever, *ever* let the ANN see testing data while training!\n",
    "\n",
    "## What sort of forest is this?\n",
    "Let's jump in with a dataset called [Covertype](https://archive.ics.uci.edu/ml/datasets/Covertype), where we try to predict forest cover type based on a number of features of a 30x30m area of forest as follows:\n",
    "\n",
    "| Column | Feature | Units | Description | How measured |\n",
    "|---|--------|-------|-------------|--------------|\n",
    "| 1 | Aspect | degrees azimuth | Aspect in degrees azimuth | Quantitative |\n",
    "| 2 | Slope | degrees | Slope in degrees | Quantitative |\n",
    "| 3 | Horizontal_Distance_To_Hydrology | meters | Horz Dist to nearest surface water features | Quantitative |\n",
    "| 4 | Vertical_Distance_To_Hydrology | meters | Vert Dist to nearest surface water features | Quantitative |\n",
    "| 5 | Horizontal_Distance_To_Roadways | meters | Horz Dist to nearest roadway | Quantitative |\n",
    "| 6 | Hillshade_9am | 0 to 255 index | Hillshade index at 9am, summer solstice | Quantitative |\n",
    "| 7 | Hillshade_Noon | 0 to 255 index | Hillshade index at noon, summer soltice | Quantitative |\n",
    "| 8 | Hillshade_3pm | 0 to 255 index | Hillshade index at 3pm, summer solstice | Quantitative |\n",
    "| 9 | Horizontal_Distance_To_Fire_Points | meters | Horz Dist to nearest wildfire ignition points | Quantitative |\n",
    "| 10-14 | Wilderness_Area | 4 binary columns with 0 (absence) or 1 (presence) | Which wilderness area this plot is in | Qualitative |\n",
    "| 14-54 | Soil_Type | 40 binary columns with 0 (absence) or 1 (presence) | Soil Type designation | Qualitative |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this information, we are trying to classify each 30x30m plot as one of seven forest types.\n",
    "\n",
    "This dataset is built into Scikit, so we can use it to download and load the dataset for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fetch_covtype in module sklearn.datasets.covtype:\n",
      "\n",
      "fetch_covtype(data_home=None, download_if_missing=True, random_state=None, shuffle=False, return_X_y=False)\n",
      "    Load the covertype dataset (classification).\n",
      "    \n",
      "    Download it if necessary.\n",
      "    \n",
      "    =================   ============\n",
      "    Classes                        7\n",
      "    Samples total             581012\n",
      "    Dimensionality                54\n",
      "    Features                     int\n",
      "    =================   ============\n",
      "    \n",
      "    Read more in the :ref:`User Guide <covtype_dataset>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data_home : string, optional\n",
      "        Specify another download and cache folder for the datasets. By default\n",
      "        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "    \n",
      "    download_if_missing : boolean, default=True\n",
      "        If False, raise a IOError if the data is not locally available\n",
      "        instead of trying to download the data from the source site.\n",
      "    \n",
      "    random_state : int, RandomState instance or None (default)\n",
      "        Determines random number generation for dataset shuffling. Pass an int\n",
      "        for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "    \n",
      "    shuffle : bool, default=False\n",
      "        Whether to shuffle dataset.\n",
      "    \n",
      "    return_X_y : boolean, default=False.\n",
      "        If True, returns ``(data.data, data.target)`` instead of a Bunch\n",
      "        object.\n",
      "    \n",
      "        .. versionadded:: 0.20\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    dataset : dict-like object with the following attributes:\n",
      "    \n",
      "    dataset.data : numpy array of shape (581012, 54)\n",
      "        Each row corresponds to the 54 features in the dataset.\n",
      "    \n",
      "    dataset.target : numpy array of shape (581012,)\n",
      "        Each value corresponds to one of the 7 forest covertypes with values\n",
      "        ranging between 1 to 7.\n",
      "    \n",
      "    dataset.DESCR : string\n",
      "        Description of the forest covertype dataset.\n",
      "    \n",
      "    (data, target) : tuple if ``return_X_y`` is True\n",
      "    \n",
      "        .. versionadded:: 0.20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "help(datasets.fetch_covtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we don't need to provide any arguments, but it warns us that it may need to download this dataset. It also describes the the returned dataset object will have the following properties:\n",
    "- .data: a numpy array with the features.\n",
    "- .target: a numpy array with the target labels. Note that each plot is classified into only one of these values.\n",
    "- .DESCR: describe this forest covertype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  [[3.066e+03 4.700e+01 9.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [3.106e+03 2.390e+02 1.500e+01 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.975e+03 2.510e+02 1.000e+01 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " ...\n",
      " [3.317e+03 1.420e+02 2.700e+01 ... 0.000e+00 1.000e+00 0.000e+00]\n",
      " [3.218e+03 9.000e+01 3.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [3.210e+03 7.600e+01 1.700e+01 ... 0.000e+00 0.000e+00 0.000e+00]]\n",
      "Data shape:  (581012, 54)\n"
     ]
    }
   ],
   "source": [
    "covtype = datasets.fetch_covtype(shuffle=True)\n",
    "print(\"Data: \", covtype.data)\n",
    "print(\"Data shape: \", covtype.data.shape) # Describe the size of this array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:  [1 1 2 ... 1 1 2]\n",
      "Target shape:  (581012,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Target: \", covtype.target)\n",
    "print(\"Target shape: \", covtype.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description:  .. _covtype_dataset:\n",
      "\n",
      "Forest covertypes\n",
      "-----------------\n",
      "\n",
      "The samples in this dataset correspond to 30Ã—30m patches of forest in the US,\n",
      "collected for the task of predicting each patch's cover type,\n",
      "i.e. the dominant species of tree.\n",
      "There are seven covertypes, making this a multiclass classification problem.\n",
      "Each sample has 54 features, described on the\n",
      "`dataset's homepage <http://archive.ics.uci.edu/ml/datasets/Covertype>`__.\n",
      "Some of the features are boolean indicators,\n",
      "while others are discrete or continuous measurements.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   ============\n",
      "    Classes                        7\n",
      "    Samples total             581012\n",
      "    Dimensionality                54\n",
      "    Features                     int\n",
      "    =================   ============\n",
      "\n",
      ":func:`sklearn.datasets.fetch_covtype` will load the covertype dataset;\n",
      "it returns a dictionary-like object\n",
      "with the feature matrix in the ``data`` member\n",
      "and the target values in ``target``.\n",
      "The dataset will be downloaded from the web if necessary.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Description: \", covtype.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data for training, testing data for testing, and validation data for validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data shape:  (50000, 54)\n",
      "Validation labels shape:  (50000,)\n",
      "Test data shape:  (250000, 54)\n",
      "Test labels shape:  (250000,)\n",
      "Training data shape:  (281012, 54)\n",
      "Training labels shape:  (281012,)\n"
     ]
    }
   ],
   "source": [
    "# Out of 581,012 data entries, let's hold back:\n",
    "# - 50,000 records as our validation dataset\n",
    "# - 250,000 records as our test dataset\n",
    "# - remaining records as our training dataset\n",
    "\n",
    "validation_data = covtype.data[0:50_000]\n",
    "validation_labels = covtype.target[0:50_000]\n",
    "\n",
    "test_data = covtype.data[50_000:300_000]\n",
    "test_labels = covtype.target[50_000:300_000]\n",
    "\n",
    "train_data = covtype.data[300_000:]\n",
    "train_labels = covtype.target[300_000:]\n",
    "\n",
    "print(\"Validation data shape: \", validation_data.shape)\n",
    "print(\"Validation labels shape: \", validation_labels.shape)\n",
    "\n",
    "print(\"Test data shape: \", test_data.shape)\n",
    "print(\"Test labels shape: \", test_labels.shape)\n",
    "\n",
    "print(\"Training data shape: \", train_data.shape)\n",
    "print(\"Training labels shape: \", train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is ready for processing. But remember that we have a variety of different input types: binary (0, 1), continuous in small ranges (0-255) and in large ranges (elevations). Before we process this data, we should normalize them into a standard range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.14495808 -0.44460642 -0.01483259 -1.12871848 -0.95158403 -1.14205634\n",
      "  1.1539444  -0.06448008 -1.00587464 -0.1625825   1.10680138 -0.23269186\n",
      " -0.87887473 -0.26017494 -0.07323182 -0.11580935 -0.09114007 -0.14768141\n",
      " -0.0528258  -0.10609663 -0.01178148 -0.01819495 -0.0444448  -0.24423533\n",
      " -0.14848449 -0.23403979 -0.17405532 -0.03180653  0.         -0.06989097\n",
      " -0.07593357 -0.05607992 -0.08400155 -0.12647098 -0.03850401 -0.24617625\n",
      " -0.33199924 -0.1942708  -0.02905327 -0.06761817 -0.04355168 -0.04062432\n",
      "  2.01133316 -0.23438637 -0.21600486 -0.31576769 -0.28939566 -0.0532979\n",
      " -0.05668334 -0.0148553  -0.02310996 -0.1657519  -0.15665416 -0.1247316 ]\n",
      "[ 0.18782869 -0.77477467  0.11871014  0.32895147 -0.34977489  0.2451638\n",
      "  0.85484102 -0.77104378 -1.11038191 -0.35210596 -0.90350447 -0.23269186\n",
      "  1.13781858 -0.26017494 -0.07323182 -0.11580935 -0.09114007 -0.14768141\n",
      " -0.0528258  -0.10609663 -0.01178148 -0.01819495 -0.0444448  -0.24423533\n",
      " -0.14848449 -0.23403979 -0.17405532 -0.03180653  0.         -0.06989097\n",
      " -0.07593357 -0.05607992 -0.08400155 -0.12647098 -0.03850401 -0.24617625\n",
      " -0.33199924 -0.1942708  -0.02905327 -0.06761817 -0.04355168 -0.04062432\n",
      " -0.49718267 -0.23438637 -0.21600486  3.16688511 -0.28939566 -0.0532979\n",
      " -0.05668334 -0.0148553  -0.02310996 -0.1657519  -0.15665416 -0.1247316 ]\n",
      "[ 0.38074642 -0.97109094 -0.68254623 -0.76076296 -0.79683311  0.96185148\n",
      "  0.44357386 -0.16541775 -0.3265774  -0.97957204  1.10680138 -0.23269186\n",
      " -0.87887473 -0.26017494 -0.07323182 -0.11580935 -0.09114007 -0.14768141\n",
      " -0.0528258  -0.10609663 -0.01178148 -0.01819495 -0.0444448  -0.24423533\n",
      " -0.14848449 -0.23403979 -0.17405532 -0.03180653  0.         -0.06989097\n",
      " -0.07593357 -0.05607992 -0.08400155 -0.12647098 -0.03850401 -0.24617625\n",
      "  3.01205507 -0.1942708  -0.02905327 -0.06761817 -0.04355168 -0.04062432\n",
      " -0.49718267 -0.23438637 -0.21600486 -0.31576769 -0.28939566 -0.0532979\n",
      " -0.05668334 -0.0148553  -0.02310996 -0.1657519  -0.15665416 -0.1247316 ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Figure out how to scale all the input features in the training dataset.\n",
    "scaler.fit(train_data)\n",
    "scaled_train_data = scaler.transform(train_data)\n",
    "print(scaled_train_data[0])\n",
    "\n",
    "# Also tranform our validation and testing data in the same way.\n",
    "scaled_test_data = scaler.transform(test_data)\n",
    "print(scaled_test_data[0])\n",
    "\n",
    "scaled_validation_data = scaler.transform(validation_data)\n",
    "print(scaled_validation_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.63656137\n",
      "Validation score: 0.757277\n",
      "Iteration 2, loss = 0.52483717\n",
      "Validation score: 0.780443\n",
      "Iteration 3, loss = 0.48668358\n",
      "Validation score: 0.795175\n",
      "Iteration 4, loss = 0.46069857\n",
      "Validation score: 0.805530\n",
      "Iteration 5, loss = 0.44184685\n",
      "Validation score: 0.808305\n",
      "Iteration 6, loss = 0.42752549\n",
      "Validation score: 0.818696\n",
      "Iteration 7, loss = 0.41688971\n",
      "Validation score: 0.821863\n",
      "Iteration 8, loss = 0.40834953\n",
      "Validation score: 0.824354\n",
      "Iteration 9, loss = 0.40131312\n",
      "Validation score: 0.826774\n",
      "Iteration 10, loss = 0.39367988\n",
      "Validation score: 0.835599\n",
      "Iteration 11, loss = 0.38872091\n",
      "Validation score: 0.831151\n",
      "Iteration 12, loss = 0.38319309\n",
      "Validation score: 0.824496\n",
      "Iteration 13, loss = 0.37791768\n",
      "Validation score: 0.838624\n",
      "Iteration 14, loss = 0.37328673\n",
      "Validation score: 0.845136\n",
      "Iteration 15, loss = 0.37011426\n",
      "Validation score: 0.845598\n",
      "Iteration 16, loss = 0.36716183\n",
      "Validation score: 0.851149\n",
      "Iteration 17, loss = 0.36332916\n",
      "Validation score: 0.846737\n",
      "Iteration 18, loss = 0.35932516\n",
      "Validation score: 0.845598\n",
      "Iteration 19, loss = 0.35647985\n",
      "Validation score: 0.851292\n",
      "Iteration 20, loss = 0.35416563\n",
      "Validation score: 0.852217\n",
      "Iteration 21, loss = 0.35229648\n",
      "Validation score: 0.850295\n",
      "Iteration 22, loss = 0.34867560\n",
      "Validation score: 0.857056\n",
      "Iteration 23, loss = 0.34654040\n",
      "Validation score: 0.849619\n",
      "Iteration 24, loss = 0.34522977\n",
      "Validation score: 0.855633\n",
      "Iteration 25, loss = 0.34189333\n",
      "Validation score: 0.856736\n",
      "Iteration 26, loss = 0.34008060\n",
      "Validation score: 0.859476\n",
      "Iteration 27, loss = 0.33849639\n",
      "Validation score: 0.853498\n",
      "Iteration 28, loss = 0.33709590\n",
      "Validation score: 0.855775\n",
      "Iteration 29, loss = 0.33418873\n",
      "Validation score: 0.852324\n",
      "Iteration 30, loss = 0.33252767\n",
      "Validation score: 0.853818\n",
      "Iteration 31, loss = 0.33139879\n",
      "Validation score: 0.859903\n",
      "Iteration 32, loss = 0.33008181\n",
      "Validation score: 0.851576\n",
      "Iteration 33, loss = 0.32886718\n",
      "Validation score: 0.858409\n",
      "Iteration 34, loss = 0.32694544\n",
      "Validation score: 0.860081\n",
      "Iteration 35, loss = 0.32545600\n",
      "Validation score: 0.863497\n",
      "Iteration 36, loss = 0.32452012\n",
      "Validation score: 0.863248\n",
      "Iteration 37, loss = 0.32295025\n",
      "Validation score: 0.862287\n",
      "Iteration 38, loss = 0.32292281\n",
      "Validation score: 0.864245\n",
      "Iteration 39, loss = 0.32105363\n",
      "Validation score: 0.861540\n",
      "Iteration 40, loss = 0.31942455\n",
      "Validation score: 0.866166\n",
      "Iteration 41, loss = 0.31921864\n",
      "Validation score: 0.865561\n",
      "Iteration 42, loss = 0.31763627\n",
      "Validation score: 0.865276\n",
      "Iteration 43, loss = 0.31686637\n",
      "Validation score: 0.865881\n",
      "Iteration 44, loss = 0.31570125\n",
      "Validation score: 0.864849\n",
      "Iteration 45, loss = 0.31465607\n",
      "Validation score: 0.862608\n",
      "Iteration 46, loss = 0.31342570\n",
      "Validation score: 0.869226\n",
      "Iteration 47, loss = 0.31215643\n",
      "Validation score: 0.865419\n",
      "Iteration 48, loss = 0.31221911\n",
      "Validation score: 0.865419\n",
      "Iteration 49, loss = 0.31133351\n",
      "Validation score: 0.860223\n",
      "Iteration 50, loss = 0.31023395\n",
      "Validation score: 0.867518\n",
      "Iteration 51, loss = 0.30943874\n",
      "Validation score: 0.861932\n",
      "Iteration 52, loss = 0.30843297\n",
      "Validation score: 0.867554\n",
      "Iteration 53, loss = 0.30709790\n",
      "Validation score: 0.868515\n",
      "Iteration 54, loss = 0.30719580\n",
      "Validation score: 0.870472\n",
      "Iteration 55, loss = 0.30608746\n",
      "Validation score: 0.870187\n",
      "Iteration 56, loss = 0.30625969\n",
      "Validation score: 0.867376\n",
      "Iteration 57, loss = 0.30508927\n",
      "Validation score: 0.867661\n",
      "Iteration 58, loss = 0.30413073\n",
      "Validation score: 0.868159\n",
      "Iteration 59, loss = 0.30280039\n",
      "Validation score: 0.873390\n",
      "Iteration 60, loss = 0.30156368\n",
      "Validation score: 0.874386\n",
      "Iteration 61, loss = 0.30074197\n",
      "Validation score: 0.875062\n",
      "Iteration 62, loss = 0.30175620\n",
      "Validation score: 0.876059\n",
      "Iteration 63, loss = 0.29961186\n",
      "Validation score: 0.873497\n",
      "Iteration 64, loss = 0.29925332\n",
      "Validation score: 0.873141\n",
      "Iteration 65, loss = 0.29917982\n",
      "Validation score: 0.875027\n",
      "Iteration 66, loss = 0.29810579\n",
      "Validation score: 0.868906\n",
      "Iteration 67, loss = 0.29800098\n",
      "Validation score: 0.874208\n",
      "Iteration 68, loss = 0.29729183\n",
      "Validation score: 0.868977\n",
      "Iteration 69, loss = 0.29693125\n",
      "Validation score: 0.873959\n",
      "Iteration 70, loss = 0.29597927\n",
      "Validation score: 0.874386\n",
      "Iteration 71, loss = 0.29550314\n",
      "Validation score: 0.870757\n",
      "Iteration 72, loss = 0.29409241\n",
      "Validation score: 0.873817\n",
      "Iteration 73, loss = 0.29411518\n",
      "Validation score: 0.876130\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 20, 20), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(\n",
    "    solver='adam', alpha=1e-5,\n",
    "    hidden_layer_sizes=(100, 20, 20),\n",
    "    batch_size='auto',\n",
    "    verbose=True,\n",
    "    early_stopping=True\n",
    ")\n",
    "clf.fit(scaled_train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87518"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(scaled_validation_data, validation_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.875876"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(scaled_test_data, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
