{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network methods\n",
    "\n",
    "Author: Gaurav Vaidya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning objectives\n",
    "* Understand what a neural network is and how it can be used.\n",
    "* Implement a neural network to label data based on multiple input features.\n",
    "* Understand when a neural network might be useful and when it might be worse than other methods.\n",
    "* Learn where to learn more about neural networks.\n",
    "\n",
    "## Learn deeply\n",
    "[Artificial Neural Networks (ANNs)](https://en.wikipedia.org/wiki/Artificial_neural_network) and [deep learning](https://en.wikipedia.org/wiki/Deep_learning) are currently getting a lot of interest, both as a subject of research and as a tool for analyzing datasets. ANNs are similar to feature crosses, except that thanks to something called [backpropagation](https://en.wikipedia.org/wiki/Backpropagation), an ANN is able to choose its own features based on the testing data. A lot of the other advantages of ANNs are related specifically to interpreting visual and auditory data, which we won't be doing today, but I'll point you some resources to learn about [convolutional neural networks](https://en.wikipedia.org/wiki/Convolutional_neural_network) yourself.\n",
    "\n",
    "### Units\n",
    "\n",
    "ANNs are designed as *layers* of *units* (or nodes, or artificial neurons). Units were initially designed based on how biological neurons function, but they're best thought of as individual *feature-crosses*. Each unit focuses on a particular aspect of the layer underneath it, and then summarizing that information for the layer above it. How does it do this?\n",
    "\n",
    "Every unit has a *weight* and a *bias*, and so is effectively doing a linear regression on the incoming data to obtain an output. But doing multiple times will just look for linear relationships in the data. To look for non-linear relationships, we need to add a non-linear function: we feed the output of the unit through an *activation function*.\n",
    "\n",
    "ANNs used to use the same sigmoid function we saw before, but these days the [rectified linear unit (ReLU) function](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) has become much more popular. It simply returns zero when the real output value is less than zero. This non-linearity can be used by the ANN to distinguish data that is not linearly separable.\n",
    "\n",
    "![ReLU and softmax rectifiers](800px-Rectifier_and_softplus_functions.svg.png )\n",
    "\n",
    "### Layers\n",
    "\n",
    "![A diagram showing input, output and hidden layers in an ANN](../nb-images/colored_neural_network.svg.png)\n",
    "\n",
    "Every neural network has three layers:\n",
    "\n",
    "* An input layer, where each unit corresponds to a particular input feature. In our example today, this will be various aspects of a forest plot, but it could be categorical data, continuous data, or even colour values from images.\n",
    "* An output layer. In many ANNs, the output layer will consist of a single label. In our example today, we will be classifying each forest plot into one of seven types, and so we will have seven output units, one for each type.\n",
    "* A hidden layer. Without a hidden layer, an ANN could only pick up linear relationships: how changes in the input layer correspond to values in the output layer. Thanks to the hidden layer, an ANN can also pick up non-linear relationships, where different groups of input values need to be treated differently to get the correct response on the output layer.\n",
    "\n",
    "Putting it all together, we end up with a type of ANN called a *multilayer perceptron (MLP) network*, which looks like the following:\n",
    "\n",
    "![A visualization of a multi-layer perceptron (MLP) from the Scikit-learn manual](../nb-images/multilayerperceptron_network.png)\n",
    "\n",
    "[Google's Machine Learning course](https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/anatomy) puts this very nicely: \"Stacking nonlinearities on nonlinearities lets us model very complicated relationships between the inputs and the predicted outputs. In brief, each layer is effectively learning a more complex, higher-level function over the raw inputs.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Backpropagation\n",
    "\n",
    "The heart of neural networks is [backpropagation algorithms](https://en.wikipedia.org/wiki/Backpropagation), which are an efficient way to \n",
    "\n",
    "In effect, an ANN is trained by:\n",
    "1. Setting all weights and biases randomly.\n",
    "2. For each row in the test data:\n",
    "    1. Set the input units to the input features.\n",
    "    2. Use unit weights and biases, passing through the activation function, to calculate the output value of each unit -- right through to the output units.\n",
    "    3. Use a loss function to Compare the output units with the expected output\n",
    "    4. Use a backpropagation algorithm to update all the weights and biases to reduce the loss.\n",
    "    \n",
    "Google has a [nice visual explanation](https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/) of backpropagation. [More detailed explanations](http://neuralnetworksanddeeplearning.com/chap2.html) are also available.\n",
    "    \n",
    "### When backpropagation goes wrong\n",
    "\n",
    "* Vanishing gradients: when weights for lower levels (closer to the input) become very small, gradients become very small too, making it hard or impossible to train these layers. The ReLU activation function can help prevent vanishing gradients.\n",
    "\n",
    "* Exploding gradients: when weights become very large, the gradients for lower layers can become very large, making it hard for these gradients to converge. Batch normalization can help prevent exploding gradients, as can lowering the learning rate.\n",
    "\n",
    "* Dead ReLU units: once the weighted sum for a ReLU activation function falls below 0, the ReLU unit can get stuck -- without an output, it doesn't contribute to the network output, and gradients can't flow through it in backpropagation. Lowering the learning rate can help keep ReLU units from dying.\n",
    "\n",
    "* Dropout regularization: in this form of regularization, a proportion of unit activations are randomly dropped out. This prevents overfitting and so helps create a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## So what do we need?\n",
    "To create an ANN, we need to choose:\n",
    "1. The number of input units (= the number of input features)\n",
    "2. The number of output units:\n",
    "    - When we are using the ANN to classify, we generally set the number of output units to the number of possible labels\n",
    "    - When we are using the ANN to calculate a regression, we generally only need a single output unit.\n",
    "3. The number of hidden layers\n",
    "    - More hidden layers allows for more complex models -- which, as you've learned, also increases your risk of overfitting! So you want to go for the simplest model that meets your needs.\n",
    "4. A loss function (the scikit-learn classes we use always use [logistic loss or cross-entropy loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html))\n",
    "4. The solver to use\n",
    "5. The regularization protocol\n",
    "\n",
    "Note that these are the [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)) of our model: we can adjust these to improve how good our model is at predicting new values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reminders of the ground rules\n",
    "* Always have training data (used to train the ANN) and testing data (used to test how well the ANN might work against data it has never seen before).\n",
    "* Never, ever, ever, ever, *ever* let the ANN see testing data while training!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN for regression: TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_diabetes in module sklearn.datasets.base:\n",
      "\n",
      "load_diabetes(return_X_y=False)\n",
      "    Load and return the diabetes dataset (regression).\n",
      "    \n",
      "    ==============      ==================\n",
      "    Samples total       442\n",
      "    Dimensionality      10\n",
      "    Features            real, -.2 < x < .2\n",
      "    Targets             integer 25 - 346\n",
      "    ==============      ==================\n",
      "    \n",
      "    Read more in the :ref:`User Guide <diabetes_dataset>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    return_X_y : boolean, default=False.\n",
      "        If True, returns ``(data, target)`` instead of a Bunch object.\n",
      "        See below for more information about the `data` and `target` object.\n",
      "    \n",
      "        .. versionadded:: 0.18\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    data : Bunch\n",
      "        Dictionary-like object, the interesting attributes are:\n",
      "        'data', the data to learn, 'target', the regression target for each\n",
      "        sample, 'data_filename', the physical location\n",
      "        of diabetes data csv dataset, and 'target_filename', the physical\n",
      "        location of diabetes targets csv datataset (added in version `0.20`).\n",
      "    \n",
      "    (data, target) : tuple if ``return_X_y`` is True\n",
      "    \n",
      "        .. versionadded:: 0.18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "help(datasets.load_diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First entry from diabetes:  [ 0.06713621 -0.04464164 -0.06117437 -0.04009932 -0.02633611 -0.02448686\n",
      "  0.03391355 -0.03949338 -0.05615757 -0.05906719]\n",
      "Shape of diabetes:  (442, 10)\n"
     ]
    }
   ],
   "source": [
    "diabetes = datasets.load_diabetes()\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "diabetes_data = shuffle(diabetes.data)\n",
    "print(\"First entry from diabetes: \", diabetes_data[0])\n",
    "print(\"Shape of diabetes: \", diabetes_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = diabetes.data[0:42]\n",
    "test_labels = diabetes.target[0:42]\n",
    "\n",
    "train_data = diabetes.data[42:]\n",
    "train_labels = diabetes.target[42:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 14608.89611676\n",
      "Validation score: -3.519580\n",
      "Iteration 2, loss = 14601.20954810\n",
      "Validation score: -3.517277\n",
      "Iteration 3, loss = 14593.57886363\n",
      "Validation score: -3.515019\n",
      "Iteration 4, loss = 14586.27267163\n",
      "Validation score: -3.512825\n",
      "Iteration 5, loss = 14579.10410784\n",
      "Validation score: -3.510664\n",
      "Iteration 6, loss = 14571.88346063\n",
      "Validation score: -3.508499\n",
      "Iteration 7, loss = 14564.73183699\n",
      "Validation score: -3.506305\n",
      "Iteration 8, loss = 14557.43467248\n",
      "Validation score: -3.504100\n",
      "Iteration 9, loss = 14550.14729699\n",
      "Validation score: -3.501958\n",
      "Iteration 10, loss = 14543.15319922\n",
      "Validation score: -3.499909\n",
      "Iteration 11, loss = 14536.53182686\n",
      "Validation score: -3.497920\n",
      "Iteration 12, loss = 14529.76952600\n",
      "Validation score: -3.495897\n",
      "Iteration 13, loss = 14523.12920888\n",
      "Validation score: -3.493813\n",
      "Iteration 14, loss = 14516.14404975\n",
      "Validation score: -3.491670\n",
      "Iteration 15, loss = 14509.07554133\n",
      "Validation score: -3.489463\n",
      "Iteration 16, loss = 14501.72610559\n",
      "Validation score: -3.487187\n",
      "Iteration 17, loss = 14494.11183226\n",
      "Validation score: -3.484838\n",
      "Iteration 18, loss = 14486.19587733\n",
      "Validation score: -3.482411\n",
      "Iteration 19, loss = 14478.19896358\n",
      "Validation score: -3.479894\n",
      "Iteration 20, loss = 14469.76143522\n",
      "Validation score: -3.477291\n",
      "Iteration 21, loss = 14461.07202349\n",
      "Validation score: -3.474621\n",
      "Iteration 22, loss = 14452.25029276\n",
      "Validation score: -3.471970\n",
      "Iteration 23, loss = 14443.68012188\n",
      "Validation score: -3.469363\n",
      "Iteration 24, loss = 14434.83148435\n",
      "Validation score: -3.466705\n",
      "Iteration 25, loss = 14425.91679930\n",
      "Validation score: -3.463942\n",
      "Iteration 26, loss = 14416.67147040\n",
      "Validation score: -3.461060\n",
      "Iteration 27, loss = 14407.10471638\n",
      "Validation score: -3.458050\n",
      "Iteration 28, loss = 14396.90329729\n",
      "Validation score: -3.454922\n",
      "Iteration 29, loss = 14386.64606118\n",
      "Validation score: -3.451655\n",
      "Iteration 30, loss = 14375.60456605\n",
      "Validation score: -3.448254\n",
      "Iteration 31, loss = 14364.35996429\n",
      "Validation score: -3.444706\n",
      "Iteration 32, loss = 14352.34059833\n",
      "Validation score: -3.441013\n",
      "Iteration 33, loss = 14340.22813835\n",
      "Validation score: -3.437156\n",
      "Iteration 34, loss = 14327.33801936\n",
      "Validation score: -3.433145\n",
      "Iteration 35, loss = 14314.13444019\n",
      "Validation score: -3.428963\n",
      "Iteration 36, loss = 14300.06805507\n",
      "Validation score: -3.424616\n",
      "Iteration 37, loss = 14285.59608074\n",
      "Validation score: -3.420086\n",
      "Iteration 38, loss = 14270.38399661\n",
      "Validation score: -3.415374\n",
      "Iteration 39, loss = 14254.76065138\n",
      "Validation score: -3.410460\n",
      "Iteration 40, loss = 14238.43361600\n",
      "Validation score: -3.405344\n",
      "Iteration 41, loss = 14221.34880177\n",
      "Validation score: -3.400022\n",
      "Iteration 42, loss = 14204.08829615\n",
      "Validation score: -3.394474\n",
      "Iteration 43, loss = 14185.37493440\n",
      "Validation score: -3.388724\n",
      "Iteration 44, loss = 14166.01540044\n",
      "Validation score: -3.382757\n",
      "Iteration 45, loss = 14146.25632250\n",
      "Validation score: -3.376548\n",
      "Iteration 46, loss = 14125.75039014\n",
      "Validation score: -3.370090\n",
      "Iteration 47, loss = 14104.38548969\n",
      "Validation score: -3.363388\n",
      "Iteration 48, loss = 14082.10776149\n",
      "Validation score: -3.356438\n",
      "Iteration 49, loss = 14059.13925143\n",
      "Validation score: -3.349229\n",
      "Iteration 50, loss = 14034.88487793\n",
      "Validation score: -3.341760\n",
      "Iteration 51, loss = 14010.35393983\n",
      "Validation score: -3.333965\n",
      "Iteration 52, loss = 13984.47085096\n",
      "Validation score: -3.325808\n",
      "Iteration 53, loss = 13957.44137170\n",
      "Validation score: -3.317300\n",
      "Iteration 54, loss = 13928.55936320\n",
      "Validation score: -3.308480\n",
      "Iteration 55, loss = 13899.53765335\n",
      "Validation score: -3.299302\n",
      "Iteration 56, loss = 13869.68048711\n",
      "Validation score: -3.289764\n",
      "Iteration 57, loss = 13837.28767698\n",
      "Validation score: -3.279910\n",
      "Iteration 58, loss = 13804.67410900\n",
      "Validation score: -3.269691\n",
      "Iteration 59, loss = 13771.08562679\n",
      "Validation score: -3.259095\n",
      "Iteration 60, loss = 13735.85197301\n",
      "Validation score: -3.248139\n",
      "Iteration 61, loss = 13698.93036859\n",
      "Validation score: -3.236829\n",
      "Iteration 62, loss = 13661.52042256\n",
      "Validation score: -3.225119\n",
      "Iteration 63, loss = 13623.20190483\n",
      "Validation score: -3.212992\n",
      "Iteration 64, loss = 13582.56450114\n",
      "Validation score: -3.200494\n",
      "Iteration 65, loss = 13540.64748442\n",
      "Validation score: -3.187603\n",
      "Iteration 66, loss = 13498.72291260\n",
      "Validation score: -3.174259\n",
      "Iteration 67, loss = 13454.61825610\n",
      "Validation score: -3.160509\n",
      "Iteration 68, loss = 13408.80185347\n",
      "Validation score: -3.146359\n",
      "Iteration 69, loss = 13362.20229678\n",
      "Validation score: -3.131775\n",
      "Iteration 70, loss = 13314.22198264\n",
      "Validation score: -3.116762\n",
      "Iteration 71, loss = 13263.61021462\n",
      "Validation score: -3.101362\n",
      "Iteration 72, loss = 13212.40951830\n",
      "Validation score: -3.085514\n",
      "Iteration 73, loss = 13160.52522769\n",
      "Validation score: -3.069186\n",
      "Iteration 74, loss = 13106.69613207\n",
      "Validation score: -3.052406\n",
      "Iteration 75, loss = 13050.94634685\n",
      "Validation score: -3.035199\n",
      "Iteration 76, loss = 12993.78681687\n",
      "Validation score: -3.017548\n",
      "Iteration 77, loss = 12935.02116773\n",
      "Validation score: -2.999453\n",
      "Iteration 78, loss = 12876.02867871\n",
      "Validation score: -2.980844\n",
      "Iteration 79, loss = 12813.27533513\n",
      "Validation score: -2.961833\n",
      "Iteration 80, loss = 12751.54257186\n",
      "Validation score: -2.942292\n",
      "Iteration 81, loss = 12686.61557531\n",
      "Validation score: -2.922320\n",
      "Iteration 82, loss = 12620.80396552\n",
      "Validation score: -2.901877\n",
      "Iteration 83, loss = 12552.67729789\n",
      "Validation score: -2.880984\n",
      "Iteration 84, loss = 12483.69271737\n",
      "Validation score: -2.859616\n",
      "Iteration 85, loss = 12413.98653380\n",
      "Validation score: -2.837744\n",
      "Iteration 86, loss = 12341.39327260\n",
      "Validation score: -2.815437\n",
      "Iteration 87, loss = 12267.60642656\n",
      "Validation score: -2.792668\n",
      "Iteration 88, loss = 12191.90932545\n",
      "Validation score: -2.769452\n",
      "Iteration 89, loss = 12115.19700047\n",
      "Validation score: -2.745751\n",
      "Iteration 90, loss = 12037.02488827\n",
      "Validation score: -2.721551\n",
      "Iteration 91, loss = 11957.47426138\n",
      "Validation score: -2.696864\n",
      "Iteration 92, loss = 11875.49359187\n",
      "Validation score: -2.671722\n",
      "Iteration 93, loss = 11792.74210310\n",
      "Validation score: -2.646085\n",
      "Iteration 94, loss = 11708.88456176\n",
      "Validation score: -2.619947\n",
      "Iteration 95, loss = 11621.40869076\n",
      "Validation score: -2.593407\n",
      "Iteration 96, loss = 11534.05390690\n",
      "Validation score: -2.566363\n",
      "Iteration 97, loss = 11444.55525646\n",
      "Validation score: -2.538857\n",
      "Iteration 98, loss = 11353.94416531\n",
      "Validation score: -2.510885\n",
      "Iteration 99, loss = 11263.65637804\n",
      "Validation score: -2.482382\n",
      "Iteration 100, loss = 11168.06741918\n",
      "Validation score: -2.453540\n",
      "Iteration 101, loss = 11073.76295852\n",
      "Validation score: -2.424235\n",
      "Iteration 102, loss = 10976.42496301\n",
      "Validation score: -2.394548\n",
      "Iteration 103, loss = 10877.68027401\n",
      "Validation score: -2.364445\n",
      "Iteration 104, loss = 10779.94885896\n",
      "Validation score: -2.333861\n",
      "Iteration 105, loss = 10678.45093889\n",
      "Validation score: -2.302898\n",
      "Iteration 106, loss = 10576.63679212\n",
      "Validation score: -2.271530\n",
      "Iteration 107, loss = 10473.92193820\n",
      "Validation score: -2.239760\n",
      "Iteration 108, loss = 10368.47756095\n",
      "Validation score: -2.207665\n",
      "Iteration 109, loss = 10263.70707753\n",
      "Validation score: -2.175157\n",
      "Iteration 110, loss = 10155.83937163\n",
      "Validation score: -2.142361\n",
      "Iteration 111, loss = 10048.08566407\n",
      "Validation score: -2.109213\n",
      "Iteration 112, loss = 9938.90250333\n",
      "Validation score: -2.075732\n",
      "Iteration 113, loss = 9827.24320628\n",
      "Validation score: -2.041989\n",
      "Iteration 114, loss = 9718.12293028\n",
      "Validation score: -2.007854\n",
      "Iteration 115, loss = 9601.11200909\n",
      "Validation score: -1.973615\n",
      "Iteration 116, loss = 9491.15933302\n",
      "Validation score: -1.938877\n",
      "Iteration 117, loss = 9376.29095312\n",
      "Validation score: -1.903902\n",
      "Iteration 118, loss = 9264.15662725\n",
      "Validation score: -1.868573\n",
      "Iteration 119, loss = 9146.50188995\n",
      "Validation score: -1.833130\n",
      "Iteration 120, loss = 9027.94588547\n",
      "Validation score: -1.797574\n",
      "Iteration 121, loss = 8912.29429259\n",
      "Validation score: -1.761708\n",
      "Iteration 122, loss = 8795.83686571\n",
      "Validation score: -1.725628\n",
      "Iteration 123, loss = 8677.28872405\n",
      "Validation score: -1.689445\n",
      "Iteration 124, loss = 8556.15665317\n",
      "Validation score: -1.653241\n",
      "Iteration 125, loss = 8438.09371898\n",
      "Validation score: -1.616852\n",
      "Iteration 126, loss = 8318.17971789\n",
      "Validation score: -1.580369\n",
      "Iteration 127, loss = 8197.19636067\n",
      "Validation score: -1.543817\n",
      "Iteration 128, loss = 8078.62015850\n",
      "Validation score: -1.507096\n",
      "Iteration 129, loss = 7957.70239351\n",
      "Validation score: -1.470367\n",
      "Iteration 130, loss = 7836.39094477\n",
      "Validation score: -1.433653\n",
      "Iteration 131, loss = 7718.23610074\n",
      "Validation score: -1.396826\n",
      "Iteration 132, loss = 7595.85594972\n",
      "Validation score: -1.360135\n",
      "Iteration 133, loss = 7473.86351860\n",
      "Validation score: -1.323545\n",
      "Iteration 134, loss = 7352.66949039\n",
      "Validation score: -1.286979\n",
      "Iteration 135, loss = 7235.66695069\n",
      "Validation score: -1.250316\n",
      "Iteration 136, loss = 7112.94420096\n",
      "Validation score: -1.213870\n",
      "Iteration 137, loss = 6993.34904550\n",
      "Validation score: -1.177516\n",
      "Iteration 138, loss = 6874.97323319\n",
      "Validation score: -1.141211\n",
      "Iteration 139, loss = 6756.06617729\n",
      "Validation score: -1.105080\n",
      "Iteration 140, loss = 6635.46381684\n",
      "Validation score: -1.069194\n",
      "Iteration 141, loss = 6519.17494405\n",
      "Validation score: -1.033428\n",
      "Iteration 142, loss = 6398.65466983\n",
      "Validation score: -0.997982\n",
      "Iteration 143, loss = 6282.79857045\n",
      "Validation score: -0.962650\n",
      "Iteration 144, loss = 6167.44802149\n",
      "Validation score: -0.927499\n",
      "Iteration 145, loss = 6055.87277751\n",
      "Validation score: -0.892450\n",
      "Iteration 146, loss = 5937.57304524\n",
      "Validation score: -0.857924\n",
      "Iteration 147, loss = 5825.00603887\n",
      "Validation score: -0.823658\n",
      "Iteration 148, loss = 5712.63036477\n",
      "Validation score: -0.789723\n",
      "Iteration 149, loss = 5602.84461464\n",
      "Validation score: -0.756103\n",
      "Iteration 150, loss = 5491.62915182\n",
      "Validation score: -0.722913\n",
      "Iteration 151, loss = 5382.63835870\n",
      "Validation score: -0.690109\n",
      "Iteration 152, loss = 5274.46407709\n",
      "Validation score: -0.657737\n",
      "Iteration 153, loss = 5169.77510447\n",
      "Validation score: -0.625703\n",
      "Iteration 154, loss = 5064.17401195\n",
      "Validation score: -0.594148\n",
      "Iteration 155, loss = 4959.72215331\n",
      "Validation score: -0.563074\n",
      "Iteration 156, loss = 4858.63095839\n",
      "Validation score: -0.532423\n",
      "Iteration 157, loss = 4760.76058637\n",
      "Validation score: -0.502163\n",
      "Iteration 158, loss = 4660.67814337\n",
      "Validation score: -0.472511\n",
      "Iteration 159, loss = 4565.07503231\n",
      "Validation score: -0.443362\n",
      "Iteration 160, loss = 4465.39030062\n",
      "Validation score: -0.414930\n",
      "Iteration 161, loss = 4375.45924451\n",
      "Validation score: -0.386838\n",
      "Iteration 162, loss = 4283.08507188\n",
      "Validation score: -0.359342\n",
      "Iteration 163, loss = 4192.91311669\n",
      "Validation score: -0.332401\n",
      "Iteration 164, loss = 4106.63912817\n",
      "Validation score: -0.305993\n",
      "Iteration 165, loss = 4017.84451699\n",
      "Validation score: -0.280312\n",
      "Iteration 166, loss = 3932.63696660\n",
      "Validation score: -0.255211\n",
      "Iteration 167, loss = 3852.39883643\n",
      "Validation score: -0.230577\n",
      "Iteration 168, loss = 3771.46663991\n",
      "Validation score: -0.206556\n",
      "Iteration 169, loss = 3692.07274492\n",
      "Validation score: -0.183166\n",
      "Iteration 170, loss = 3617.29770955\n",
      "Validation score: -0.160274\n",
      "Iteration 171, loss = 3540.43281419\n",
      "Validation score: -0.138134\n",
      "Iteration 172, loss = 3467.80849406\n",
      "Validation score: -0.116546\n",
      "Iteration 173, loss = 3398.49441432\n",
      "Validation score: -0.095506\n",
      "Iteration 174, loss = 3329.14142727\n",
      "Validation score: -0.075103\n",
      "Iteration 175, loss = 3264.78521309\n",
      "Validation score: -0.055224\n",
      "Iteration 176, loss = 3198.39546768\n",
      "Validation score: -0.036097\n",
      "Iteration 177, loss = 3134.99517101\n",
      "Validation score: -0.017597\n",
      "Iteration 178, loss = 3075.62683023\n",
      "Validation score: 0.000353\n",
      "Iteration 179, loss = 3016.29370694\n",
      "Validation score: 0.017654\n",
      "Iteration 180, loss = 2957.61863032\n",
      "Validation score: 0.034289\n",
      "Iteration 181, loss = 2906.97472208\n",
      "Validation score: 0.050537\n",
      "Iteration 182, loss = 2852.02079184\n",
      "Validation score: 0.066076\n",
      "Iteration 183, loss = 2800.37590615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score: 0.080995\n",
      "Iteration 184, loss = 2753.44585656\n",
      "Validation score: 0.095469\n",
      "Iteration 185, loss = 2705.80106407\n",
      "Validation score: 0.109325\n",
      "Iteration 186, loss = 2661.31672917\n",
      "Validation score: 0.122631\n",
      "Iteration 187, loss = 2618.40848051\n",
      "Validation score: 0.135382\n",
      "Iteration 188, loss = 2577.11781844\n",
      "Validation score: 0.147580\n",
      "Iteration 189, loss = 2536.48484047\n",
      "Validation score: 0.159192\n",
      "Iteration 190, loss = 2498.78011885\n",
      "Validation score: 0.170288\n",
      "Iteration 191, loss = 2461.72431823\n",
      "Validation score: 0.180861\n",
      "Iteration 192, loss = 2428.64401470\n",
      "Validation score: 0.191003\n",
      "Iteration 193, loss = 2395.04902464\n",
      "Validation score: 0.200622\n",
      "Iteration 194, loss = 2362.25847004\n",
      "Validation score: 0.209736\n",
      "Iteration 195, loss = 2333.96432134\n",
      "Validation score: 0.218472\n",
      "Iteration 196, loss = 2304.30621923\n",
      "Validation score: 0.226745\n",
      "Iteration 197, loss = 2277.89386145\n",
      "Validation score: 0.234644\n",
      "Iteration 198, loss = 2252.43891490\n",
      "Validation score: 0.242141\n",
      "Iteration 199, loss = 2227.38307977\n",
      "Validation score: 0.249234\n",
      "Iteration 200, loss = 2203.26004560\n",
      "Validation score: 0.255916\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 20), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "clf = MLPRegressor(\n",
    "    solver='adam',  # The solver to use\n",
    "    alpha=1e-5,     # The L2 regularization rate: higher values increase cost for larger weights\n",
    "    hidden_layer_sizes=(100,20),\n",
    "                    # \n",
    "                    # Note that we don't need to specify input and output neuron numbers: MLPClassifier\n",
    "                    # determines this based on the shape of the features and labels being fitted.\n",
    "    batch_size='auto',\n",
    "    verbose=True,\n",
    "    early_stopping=True\n",
    ")\n",
    "clf.fit(train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25701869403045385"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(scaled_test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ANN for classification: what sort of forest is this?\n",
    "Let's jump in with a dataset called [Covertype](https://archive.ics.uci.edu/ml/datasets/Covertype), where we try to predict forest cover type based on a number of features of a 30x30m area of forest as follows:\n",
    "\n",
    "| Column | Feature | Units | Description | How measured |\n",
    "|---|--------|-------|-------------|--------------|\n",
    "| 1 | Aspect | degrees azimuth | Aspect in degrees azimuth | Quantitative |\n",
    "| 2 | Slope | degrees | Slope in degrees | Quantitative |\n",
    "| 3 | Horizontal_Distance_To_Hydrology | meters | Horz Dist to nearest surface water features | Quantitative |\n",
    "| 4 | Vertical_Distance_To_Hydrology | meters | Vert Dist to nearest surface water features | Quantitative |\n",
    "| 5 | Horizontal_Distance_To_Roadways | meters | Horz Dist to nearest roadway | Quantitative |\n",
    "| 6 | Hillshade_9am | 0 to 255 index | Hillshade index at 9am, summer solstice | Quantitative |\n",
    "| 7 | Hillshade_Noon | 0 to 255 index | Hillshade index at noon, summer soltice | Quantitative |\n",
    "| 8 | Hillshade_3pm | 0 to 255 index | Hillshade index at 3pm, summer solstice | Quantitative |\n",
    "| 9 | Horizontal_Distance_To_Fire_Points | meters | Horz Dist to nearest wildfire ignition points | Quantitative |\n",
    "| 10-14 | Wilderness_Area | 4 binary columns with 0 (absence) or 1 (presence) | Which wilderness area this plot is in | Qualitative |\n",
    "| 14-54 | Soil_Type | 40 binary columns with 0 (absence) or 1 (presence) | Soil Type designation | Qualitative |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this information, we are trying to classify each 30x30m plot as one of seven forest types.\n",
    "\n",
    "This dataset is built into Scikit, so we can use it to download and load the dataset for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fetch_covtype in module sklearn.datasets.covtype:\n",
      "\n",
      "fetch_covtype(data_home=None, download_if_missing=True, random_state=None, shuffle=False, return_X_y=False)\n",
      "    Load the covertype dataset (classification).\n",
      "    \n",
      "    Download it if necessary.\n",
      "    \n",
      "    =================   ============\n",
      "    Classes                        7\n",
      "    Samples total             581012\n",
      "    Dimensionality                54\n",
      "    Features                     int\n",
      "    =================   ============\n",
      "    \n",
      "    Read more in the :ref:`User Guide <covtype_dataset>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data_home : string, optional\n",
      "        Specify another download and cache folder for the datasets. By default\n",
      "        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "    \n",
      "    download_if_missing : boolean, default=True\n",
      "        If False, raise a IOError if the data is not locally available\n",
      "        instead of trying to download the data from the source site.\n",
      "    \n",
      "    random_state : int, RandomState instance or None (default)\n",
      "        Determines random number generation for dataset shuffling. Pass an int\n",
      "        for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "    \n",
      "    shuffle : bool, default=False\n",
      "        Whether to shuffle dataset.\n",
      "    \n",
      "    return_X_y : boolean, default=False.\n",
      "        If True, returns ``(data.data, data.target)`` instead of a Bunch\n",
      "        object.\n",
      "    \n",
      "        .. versionadded:: 0.20\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    dataset : dict-like object with the following attributes:\n",
      "    \n",
      "    dataset.data : numpy array of shape (581012, 54)\n",
      "        Each row corresponds to the 54 features in the dataset.\n",
      "    \n",
      "    dataset.target : numpy array of shape (581012,)\n",
      "        Each value corresponds to one of the 7 forest covertypes with values\n",
      "        ranging between 1 to 7.\n",
      "    \n",
      "    dataset.DESCR : string\n",
      "        Description of the forest covertype dataset.\n",
      "    \n",
      "    (data, target) : tuple if ``return_X_y`` is True\n",
      "    \n",
      "        .. versionadded:: 0.20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "help(datasets.fetch_covtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we don't need to provide any arguments, but it warns us that it may need to download this dataset. It also describes the the returned dataset object will have the following properties:\n",
    "- .data: a numpy array with the features.\n",
    "- .target: a numpy array with the target labels. Note that each plot is classified into only one of these values.\n",
    "- .DESCR: describe this forest covertype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  [[2670.  108.   12. ...    0.    0.    0.]\n",
      " [3204.  253.   12. ...    0.    0.    0.]\n",
      " [3039.  320.   16. ...    0.    0.    0.]\n",
      " ...\n",
      " [2806.   43.    9. ...    0.    0.    0.]\n",
      " [3066.  283.    6. ...    0.    0.    0.]\n",
      " [3225.  208.   28. ...    0.    0.    0.]]\n",
      "Data shape:  (581012, 54)\n"
     ]
    }
   ],
   "source": [
    "covtype = datasets.fetch_covtype(shuffle=True)\n",
    "print(\"Data: \", covtype.data)\n",
    "print(\"Data shape: \", covtype.data.shape) # Describe the size of this array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:  [3 1 1 ... 1 2 1]\n",
      "Target shape:  (581012,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Target: \", covtype.target)\n",
    "print(\"Target shape: \", covtype.target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description:  .. _covtype_dataset:\n",
      "\n",
      "Forest covertypes\n",
      "-----------------\n",
      "\n",
      "The samples in this dataset correspond to 30Ã—30m patches of forest in the US,\n",
      "collected for the task of predicting each patch's cover type,\n",
      "i.e. the dominant species of tree.\n",
      "There are seven covertypes, making this a multiclass classification problem.\n",
      "Each sample has 54 features, described on the\n",
      "`dataset's homepage <http://archive.ics.uci.edu/ml/datasets/Covertype>`__.\n",
      "Some of the features are boolean indicators,\n",
      "while others are discrete or continuous measurements.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   ============\n",
      "    Classes                        7\n",
      "    Samples total             581012\n",
      "    Dimensionality                54\n",
      "    Features                     int\n",
      "    =================   ============\n",
      "\n",
      ":func:`sklearn.datasets.fetch_covtype` will load the covertype dataset;\n",
      "it returns a dictionary-like object\n",
      "with the feature matrix in the ``data`` member\n",
      "and the target values in ``target``.\n",
      "The dataset will be downloaded from the web if necessary.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Description: \", covtype.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training data for training, testing data for testing, and validation data for validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data shape:  (50000, 54)\n",
      "Validation labels shape:  (50000,)\n",
      "Test data shape:  (250000, 54)\n",
      "Test labels shape:  (250000,)\n",
      "Training data shape:  (281012, 54)\n",
      "Training labels shape:  (281012,)\n"
     ]
    }
   ],
   "source": [
    "# Out of 581,012 data entries, let's hold back:\n",
    "# - 250,000 records as our test dataset\n",
    "# - remaining records as our training dataset\n",
    "\n",
    "test_data = covtype.data[0:250_000]\n",
    "test_labels = covtype.target[0:250_000]\n",
    "\n",
    "train_data = covtype.data[250_000:]\n",
    "train_labels = covtype.target[250_000:]\n",
    "\n",
    "print(\"Validation data shape: \", validation_data.shape)\n",
    "print(\"Validation labels shape: \", validation_labels.shape)\n",
    "\n",
    "print(\"Test data shape: \", test_data.shape)\n",
    "print(\"Test labels shape: \", test_labels.shape)\n",
    "\n",
    "print(\"Training data shape: \", train_data.shape)\n",
    "print(\"Training labels shape: \", train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is ready for processing. But remember that we have a variety of different input types: binary (0, 1), continuous in small ranges (0-255) and in large ranges (elevations). Before we process this data, we should normalize them into a standard range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.60736862e+00  1.26976625e+00  2.52424211e+00 -1.32789752e-01\n",
      " -8.46317415e-01  5.45818884e-01 -3.70587203e+00 -7.77004916e-01\n",
      "  2.17874024e+00 -8.66734546e-02  1.10618082e+00 -2.32878738e-01\n",
      " -8.77654257e-01 -2.60891433e-01 -7.33790197e-02 -1.13756645e-01\n",
      " -9.06824489e-02 -1.47164828e-01 -5.28596572e-02 -1.06780531e-01\n",
      " -1.36044010e-02 -1.83896215e-02 -4.50039975e-02 -2.44733925e-01\n",
      " -1.47354014e-01 -2.33743604e-01 -1.75417877e-01 -3.15815220e-02\n",
      " -1.88641977e-03 -7.04798023e-02 -7.70864687e-02 -5.74366014e-02\n",
      " -8.33979573e-02 -1.25961739e-01 -3.82249167e-02 -2.47644451e-01\n",
      " -3.31516804e-01 -1.94359553e-01 -2.76708817e-02 -6.70314648e-02\n",
      " -4.31396443e-02 -4.12782423e-02 -4.99658577e-01 -2.33963654e-01\n",
      " -2.14121142e-01 -3.14156848e-01 -2.90124367e-01 -5.24519154e-02\n",
      " -5.69983904e-02 -1.57848582e-02 -2.34162292e-02 -1.65615860e-01\n",
      "  6.39371431e+00 -1.23519671e-01]\n",
      "[ 1.13253983e+00 -2.93736903e-01  2.54632485e-01 -5.64809237e-01\n",
      " -1.70261524e+00 -1.11786676e+00  1.22878378e+00  1.84171682e-01\n",
      " -9.55964832e-01  1.23364011e+00 -9.04011336e-01  4.29408030e+00\n",
      " -8.77654257e-01 -2.60891433e-01 -7.33790197e-02 -1.13756645e-01\n",
      " -9.06824489e-02 -1.47164828e-01 -5.28596572e-02 -1.06780531e-01\n",
      " -1.36044010e-02 -1.83896215e-02 -4.50039975e-02 -2.44733925e-01\n",
      " -1.47354014e-01 -2.33743604e-01 -1.75417877e-01 -3.15815220e-02\n",
      " -1.88641977e-03 -7.04798023e-02 -7.70864687e-02 -5.74366014e-02\n",
      " -8.33979573e-02 -1.25961739e-01 -3.82249167e-02 -2.47644451e-01\n",
      " -3.31516804e-01 -1.94359553e-01  3.61390725e+01 -6.70314648e-02\n",
      " -4.31396443e-02 -4.12782423e-02 -4.99658577e-01 -2.33963654e-01\n",
      " -2.14121142e-01 -3.14156848e-01 -2.90124367e-01 -5.24519154e-02\n",
      " -5.69983904e-02 -1.57848582e-02 -2.34162292e-02 -1.65615860e-01\n",
      " -1.56403610e-01 -1.23519671e-01]\n",
      "[-1.03453594e+00 -4.27751459e-01 -2.79393310e-01  1.66103028e+00\n",
      " -3.83916589e-01 -1.03904036e+00  1.07924876e+00  8.29951982e-02\n",
      " -7.99229578e-01 -5.65004198e-01 -9.04011336e-01 -2.32878738e-01\n",
      "  1.13940084e+00 -2.60891433e-01 -7.33790197e-02 -1.13756645e-01\n",
      " -9.06824489e-02 -1.47164828e-01 -5.28596572e-02 -1.06780531e-01\n",
      " -1.36044010e-02 -1.83896215e-02 -4.50039975e-02 -2.44733925e-01\n",
      "  6.78637775e+00 -2.33743604e-01 -1.75417877e-01 -3.15815220e-02\n",
      " -1.88641977e-03 -7.04798023e-02 -7.70864687e-02 -5.74366014e-02\n",
      " -8.33979573e-02 -1.25961739e-01 -3.82249167e-02 -2.47644451e-01\n",
      " -3.31516804e-01 -1.94359553e-01 -2.76708817e-02 -6.70314648e-02\n",
      " -4.31396443e-02 -4.12782423e-02 -4.99658577e-01 -2.33963654e-01\n",
      " -2.14121142e-01 -3.14156848e-01 -2.90124367e-01 -5.24519154e-02\n",
      " -5.69983904e-02 -1.57848582e-02 -2.34162292e-02 -1.65615860e-01\n",
      " -1.56403610e-01 -1.23519671e-01]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Figure out how to scale all the input features in the training dataset.\n",
    "scaler.fit(train_data)\n",
    "scaled_train_data = scaler.transform(train_data)\n",
    "print(scaled_train_data[0])\n",
    "\n",
    "# Also tranform our validation and testing data in the same way.\n",
    "scaled_test_data = scaler.transform(test_data)\n",
    "print(scaled_test_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying among multiple categories\n",
    "\n",
    "Having multiple output units usually would result in each unit being considered independently, allowing you to assign multiple labels for a particular input (for instance, a single image might be classified as containing both a cloud as well as a bird). However, we use scikit-learn's [MLPClassifier](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification), which automatically uses *softmax* to treat labels as exclusive to each other.\n",
    "\n",
    "### The power of softmax\n",
    "\n",
    "As based on [](https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax)\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.66653904\n",
      "Validation score: 0.749413\n",
      "Iteration 2, loss = 0.56445348\n",
      "Validation score: 0.765035\n",
      "Iteration 3, loss = 0.53419656\n",
      "Validation score: 0.777169\n",
      "Iteration 4, loss = 0.51499368\n",
      "Validation score: 0.780941\n",
      "Iteration 5, loss = 0.50032446\n",
      "Validation score: 0.787311\n",
      "Iteration 6, loss = 0.48875770\n",
      "Validation score: 0.794285\n",
      "Iteration 7, loss = 0.47942608\n",
      "Validation score: 0.797452\n",
      "Iteration 8, loss = 0.47112352\n",
      "Validation score: 0.795815\n",
      "Iteration 9, loss = 0.46449897\n",
      "Validation score: 0.805388\n",
      "Iteration 10, loss = 0.45811287\n",
      "Validation score: 0.808590\n",
      "Iteration 11, loss = 0.45244680\n",
      "Validation score: 0.811330\n",
      "Iteration 12, loss = 0.44708846\n",
      "Validation score: 0.809978\n",
      "Iteration 13, loss = 0.44114382\n",
      "Validation score: 0.816526\n",
      "Iteration 14, loss = 0.43669484\n",
      "Validation score: 0.812825\n",
      "Iteration 15, loss = 0.43251937\n",
      "Validation score: 0.819088\n",
      "Iteration 16, loss = 0.42858463\n",
      "Validation score: 0.816170\n",
      "Iteration 17, loss = 0.42432445\n",
      "Validation score: 0.821543\n",
      "Iteration 18, loss = 0.42102559\n",
      "Validation score: 0.823002\n",
      "Iteration 19, loss = 0.41765650\n",
      "Validation score: 0.823180\n",
      "Iteration 20, loss = 0.41507193\n",
      "Validation score: 0.826205\n",
      "Iteration 21, loss = 0.41258396\n",
      "Validation score: 0.825493\n",
      "Iteration 22, loss = 0.40932842\n",
      "Validation score: 0.823963\n",
      "Iteration 23, loss = 0.40720972\n",
      "Validation score: 0.824069\n",
      "Iteration 24, loss = 0.40545201\n",
      "Validation score: 0.830332\n",
      "Iteration 25, loss = 0.40276431\n",
      "Validation score: 0.828553\n",
      "Iteration 26, loss = 0.40089627\n",
      "Validation score: 0.830937\n",
      "Iteration 27, loss = 0.39842909\n",
      "Validation score: 0.832040\n",
      "Iteration 28, loss = 0.39725575\n",
      "Validation score: 0.830012\n",
      "Iteration 29, loss = 0.39482908\n",
      "Validation score: 0.829016\n",
      "Iteration 30, loss = 0.39373561\n",
      "Validation score: 0.834033\n",
      "Iteration 31, loss = 0.39192913\n",
      "Validation score: 0.833215\n",
      "Iteration 32, loss = 0.39050465\n",
      "Validation score: 0.837200\n",
      "Iteration 33, loss = 0.38933214\n",
      "Validation score: 0.832183\n",
      "Iteration 34, loss = 0.38730001\n",
      "Validation score: 0.837841\n",
      "Iteration 35, loss = 0.38658284\n",
      "Validation score: 0.837734\n",
      "Iteration 36, loss = 0.38483339\n",
      "Validation score: 0.837663\n",
      "Iteration 37, loss = 0.38335438\n",
      "Validation score: 0.839157\n",
      "Iteration 38, loss = 0.38120096\n",
      "Validation score: 0.838374\n",
      "Iteration 39, loss = 0.38139668\n",
      "Validation score: 0.840616\n",
      "Iteration 40, loss = 0.37978784\n",
      "Validation score: 0.840794\n",
      "Iteration 41, loss = 0.37827325\n",
      "Validation score: 0.840260\n",
      "Iteration 42, loss = 0.37778493\n",
      "Validation score: 0.840937\n",
      "Iteration 43, loss = 0.37614332\n",
      "Validation score: 0.840154\n",
      "Iteration 44, loss = 0.37565404\n",
      "Validation score: 0.843961\n",
      "Iteration 45, loss = 0.37431509\n",
      "Validation score: 0.842751\n",
      "Iteration 46, loss = 0.37317442\n",
      "Validation score: 0.842787\n",
      "Iteration 47, loss = 0.37186811\n",
      "Validation score: 0.842218\n",
      "Iteration 48, loss = 0.37194536\n",
      "Validation score: 0.841791\n",
      "Iteration 49, loss = 0.37039372\n",
      "Validation score: 0.845990\n",
      "Iteration 50, loss = 0.36943000\n",
      "Validation score: 0.844068\n",
      "Iteration 51, loss = 0.36917659\n",
      "Validation score: 0.843392\n",
      "Iteration 52, loss = 0.36767867\n",
      "Validation score: 0.842218\n",
      "Iteration 53, loss = 0.36754552\n",
      "Validation score: 0.845171\n",
      "Iteration 54, loss = 0.36701336\n",
      "Validation score: 0.843605\n",
      "Iteration 55, loss = 0.36585993\n",
      "Validation score: 0.843855\n",
      "Iteration 56, loss = 0.36598494\n",
      "Validation score: 0.842573\n",
      "Iteration 57, loss = 0.36398808\n",
      "Validation score: 0.843677\n",
      "Iteration 58, loss = 0.36457609\n",
      "Validation score: 0.847022\n",
      "Iteration 59, loss = 0.36359993\n",
      "Validation score: 0.845705\n",
      "Iteration 60, loss = 0.36212274\n",
      "Validation score: 0.847377\n",
      "Iteration 61, loss = 0.36265437\n",
      "Validation score: 0.844032\n",
      "Iteration 62, loss = 0.36197736\n",
      "Validation score: 0.849370\n",
      "Iteration 63, loss = 0.36082264\n",
      "Validation score: 0.846345\n",
      "Iteration 64, loss = 0.36002757\n",
      "Validation score: 0.846630\n",
      "Iteration 65, loss = 0.36018484\n",
      "Validation score: 0.849975\n",
      "Iteration 66, loss = 0.35932607\n",
      "Validation score: 0.847627\n",
      "Iteration 67, loss = 0.35834669\n",
      "Validation score: 0.846986\n",
      "Iteration 68, loss = 0.35762633\n",
      "Validation score: 0.837947\n",
      "Iteration 69, loss = 0.35715269\n",
      "Validation score: 0.849157\n",
      "Iteration 70, loss = 0.35700785\n",
      "Validation score: 0.850011\n",
      "Iteration 71, loss = 0.35661477\n",
      "Validation score: 0.847306\n",
      "Iteration 72, loss = 0.35602385\n",
      "Validation score: 0.845278\n",
      "Iteration 73, loss = 0.35605402\n",
      "Validation score: 0.839869\n",
      "Iteration 74, loss = 0.35482422\n",
      "Validation score: 0.850189\n",
      "Iteration 75, loss = 0.35432135\n",
      "Validation score: 0.849477\n",
      "Iteration 76, loss = 0.35451236\n",
      "Validation score: 0.851292\n",
      "Iteration 77, loss = 0.35375334\n",
      "Validation score: 0.848801\n",
      "Iteration 78, loss = 0.35282343\n",
      "Validation score: 0.849014\n",
      "Iteration 79, loss = 0.35298734\n",
      "Validation score: 0.845278\n",
      "Iteration 80, loss = 0.35216014\n",
      "Validation score: 0.851292\n",
      "Iteration 81, loss = 0.35195336\n",
      "Validation score: 0.851114\n",
      "Iteration 82, loss = 0.35084219\n",
      "Validation score: 0.850687\n",
      "Iteration 83, loss = 0.35144714\n",
      "Validation score: 0.847271\n",
      "Iteration 84, loss = 0.35039704\n",
      "Validation score: 0.848196\n",
      "Iteration 85, loss = 0.34957906\n",
      "Validation score: 0.848196\n",
      "Iteration 86, loss = 0.34925613\n",
      "Validation score: 0.847804\n",
      "Iteration 87, loss = 0.34915559\n",
      "Validation score: 0.850153\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "       hidden_layer_sizes=100, learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "clf = MLPClassifier(\n",
    "    solver='adam',  # The solver to use\n",
    "    alpha=1e-5,     # The L2 regularization rate: higher values increase cost for larger weights\n",
    "    hidden_layer_sizes=(100),\n",
    "                    # \n",
    "                    # Note that we don't need to specify input and output neuron numbers: MLPClassifier\n",
    "                    # determines this based on the shape of the features and labels being fitted.\n",
    "    batch_size='auto',\n",
    "    verbose=True,\n",
    "    early_stopping=True\n",
    ")\n",
    "clf.fit(scaled_train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87952"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(scaled_test_data, test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
