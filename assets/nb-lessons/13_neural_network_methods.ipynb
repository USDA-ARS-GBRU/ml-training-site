{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network methods\n",
    "\n",
    "Author: Gaurav Vaidya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Learning objectives\n",
    "* Understand what an artificial neural network (ANN) is and how it can be used.\n",
    "* Implement ANNs for use in prediction and classification based on multiple input features.\n",
    "\n",
    "## Learning deeply\n",
    "[Artificial Neural Networks (ANNs)](https://en.wikipedia.org/wiki/Artificial_neural_network) and [deep learning](https://en.wikipedia.org/wiki/Deep_learning) are currently getting a lot of interest, both as a subject of research and as a tool for analyzing datasets. A big difference from other machine learning techniques we've looked at so far is that ANNs can identify characteristics of interest by themselves, rather than having to be chosen by data scientists. Some of the other advantages of ANNs are related specifically to interpreting video and audio data, such as by using [convolutional neural networks](https://en.wikipedia.org/wiki/Convolutional_neural_network), but today we will focus on simple ANNs so you understand their struction and function.\n",
    "\n",
    "### Units\n",
    "\n",
    "ANNs are designed as *layers* of *units* (or nodes, or [artificial neurons](https://en.wikipedia.org/wiki/Artificial_neuron)). Each unit accepts multiple inputs, each of which has a different weight, including one bias input, which it combines into a single value. That single value is passed to an [activation function](https://en.wikipedia.org/wiki/Activation_function), which provides an output only if the combined value is greater than a particular threshold (usually, zero).\n",
    "\n",
    "In effect, each unit focuses on a particular aspect of the layer underneath it, and then summarizing that information for the layer above it. Every input to every unit has a *weight* and the unit has a *bias* input, and so is effectively doing a linear regression on the incoming data to obtain an output. The use of a non-linear activation function allows the ANN to predict and classify data that are not [linearly separable](https://en.wikipedia.org/wiki/Linear_separability). ANNs used to use the same sigmoid function we saw before, but these days the [rectified linear unit (ReLU) function](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) has become much more popular. It simply returns zero when the real output value is less than zero.\n",
    "\n",
    "![ReLU and softmax rectifiers](800px-Rectifier_and_softplus_functions.svg.png )\n",
    "\n",
    "### Layers\n",
    "\n",
    "![A diagram showing input, output and hidden layers in an ANN](../nb-images/colored_neural_network.svg.png)\n",
    "\n",
    "Every neural network has three layers:\n",
    "\n",
    "* An input layer, where each unit corresponds to a particular input feature. This could be categorical data, continuous data, or even colour values from images.\n",
    "* An output layer. We will be running two examples today: in the first, we will use a single output unit (the predicted price for a particular house in California). In the second, we will use seven output units, each corresponding to a particular type of forest cover.\n",
    "* A hidden layer. Without a hidden layer, an ANN can only pick up linear relationships: how changes in the input layer correspond to values in the output layer. Thanks to the hidden layer, an ANN can also pick up non-linear relationships, where different groups of input values interact in complicated ways to get the correct response on the output layer. The \"deep\" in [deep learning](https://en.wikipedia.org/wiki/Deep_learning) refers to the hidden layers that allow the model to identify intermediate patterns between the input and output layers.\n",
    "\n",
    "Putting it all together, we end up with a type of ANN called a *multilayer perceptron (MLP) network*, which looks like the following:\n",
    "\n",
    "![A visualization of a multi-layer perceptron (MLP) from the Scikit-learn manual](../nb-images/multilayerperceptron_network.png)\n",
    "\n",
    "[Google's Machine Learning course](https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/anatomy) puts this very nicely: \"Stacking nonlinearities on nonlinearities lets us model very complicated relationships between the inputs and the predicted outputs. In brief, each layer is effectively learning a more complex, higher-level function over the raw inputs.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## So what do we need?\n",
    "To create an ANN, we need to choose:\n",
    "1. The number of input units (= the number of input features)\n",
    "2. The number of output units:\n",
    "    - When using the ANN to predict, we generally only need a single output unit.\n",
    "    - When using the ANN to classify, we generally set the number of output units to the number of possible labels.\n",
    "3. The number of hidden layers\n",
    "    - More hidden layers allows for more complex models -- which, as you've learned, also increases your risk of overfitting! So you want to go for the simplest model that meets your needs.\n",
    "4. A loss function. The scikit-learn classes we use always use [logistic loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html), also known as cross-entropy loss.\n",
    "4. The solver to use. The solver controls learning by searching for local minima in the parameter space. ANNs generally use [stochastic gradient descent (SGD)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent) such as [RMSProp](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#RMSProp), but today we will use [Adaptive Moment Estimation (Adam)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam).\n",
    "5. The regularization protocol. We will use L2 regularization.\n",
    "\n",
    "Note that these are the [hyperparameters](https://en.wikipedia.org/wiki/Hyperparameter_(machine_learning)) of our model: we will adjust these hyperparameters to improve how quickly and accurately we can determine the actual parameters of our model, which is the set of weights and biases on all units across all layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Reminders of the ground rules\n",
    "* Always have *training* data and *testing* data, and make sure the ANN *never* sees the testing data.\n",
    "* It's usually a good idea to work with shuffled data.\n",
    "* ANNs aren't good when the data is in different ranges: it's usually a good idea to normalize it before use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANN for prediction: how much might this house cost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aren't very good datasets for showcasing prediction on biological data, so we will use one of the classic machine learning datasets: a dataset of [California house prices](https://www.kaggle.com/camnugent/california-housing-prices), based on the 1990 census and published in [Pace and Barry, 1997](https://doi.org/10.1016/S0167-7152(96)00140-X). Scikit-Learn can download this dataset for us, so let's start with that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fetch_california_housing in module sklearn.datasets.california_housing:\n",
      "\n",
      "fetch_california_housing(data_home=None, download_if_missing=True, return_X_y=False)\n",
      "    Load the California housing dataset (regression).\n",
      "    \n",
      "    ==============     ==============\n",
      "    Samples total               20640\n",
      "    Dimensionality                  8\n",
      "    Features                     real\n",
      "    Target             real 0.15 - 5.\n",
      "    ==============     ==============\n",
      "    \n",
      "    Read more in the :ref:`User Guide <california_housing_dataset>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data_home : optional, default: None\n",
      "        Specify another download and cache folder for the datasets. By default\n",
      "        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "    \n",
      "    download_if_missing : optional, default=True\n",
      "        If False, raise a IOError if the data is not locally available\n",
      "        instead of trying to download the data from the source site.\n",
      "    \n",
      "    \n",
      "    return_X_y : boolean, default=False.\n",
      "        If True, returns ``(data.data, data.target)`` instead of a Bunch\n",
      "        object.\n",
      "    \n",
      "        .. versionadded:: 0.20\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    dataset : dict-like object with the following attributes:\n",
      "    \n",
      "    dataset.data : ndarray, shape [20640, 8]\n",
      "        Each row corresponding to the 8 feature values in order.\n",
      "    \n",
      "    dataset.target : numpy array of shape (20640,)\n",
      "        Each value corresponds to the average house value in units of 100,000.\n",
      "    \n",
      "    dataset.feature_names : array of length 8\n",
      "        Array of ordered feature names used in the dataset.\n",
      "    \n",
      "    dataset.DESCR : string\n",
      "        Description of the California housing dataset.\n",
      "    \n",
      "    (data, target) : tuple if ``return_X_y`` is True\n",
      "    \n",
      "        .. versionadded:: 0.20\n",
      "    \n",
      "    Notes\n",
      "    ------\n",
      "    \n",
      "    This dataset consists of 20,640 samples and 9 features.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "help(datasets.fetch_california_housing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of California housing data:  (20640, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1         2         3       4         5      6       7\n",
       "0  8.3252  41.0  6.984127  1.023810   322.0  2.555556  37.88 -122.23\n",
       "1  8.3014  21.0  6.238137  0.971880  2401.0  2.109842  37.86 -122.22\n",
       "2  7.2574  52.0  8.288136  1.073446   496.0  2.802260  37.85 -122.24\n",
       "3  5.6431  52.0  5.817352  1.073059   558.0  2.547945  37.85 -122.25\n",
       "4  3.8462  52.0  6.281853  1.081081   565.0  2.181467  37.85 -122.25"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Fetch California housing dataset. This will be downloaded to your computer.\n",
    "calif = datasets.fetch_california_housing()\n",
    "print(\"Shape of California housing data: \", calif.data.shape)\n",
    "califdf = pd.DataFrame.from_records(calif.data)\n",
    "califdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by shuffling our data and splitting into testing data (640 rows) and training data (the remaining 20,000 rows)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape:  (640, 8)\n",
      "Test label shape:  (640,)\n",
      "Train data shape:  (20000, 8)\n",
      "Train label shape:  (20000,)\n"
     ]
    }
   ],
   "source": [
    "# SkLearn has a shuffle method that can shuffle two arrays together,\n",
    "# allowing us to shuffle both the features and the known labels without\n",
    "# disrupting their order.\n",
    "from sklearn.utils import shuffle\n",
    "calif_data, calif_labels = shuffle(calif.data, calif.target)\n",
    "\n",
    "# Use a portion of the data as testing data.\n",
    "test_data = calif_data[0:640]\n",
    "test_labels = calif_labels[0:640]\n",
    "print(\"Test data shape: \", test_data.shape)\n",
    "print(\"Test label shape: \", test_labels.shape)\n",
    "\n",
    "# Use the remaining data as training data\n",
    "train_data = calif_data[640:]\n",
    "train_labels = calif_labels[640:]\n",
    "print(\"Train data shape: \", train_data.shape)\n",
    "print(\"Train label shape: \", train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "      <td>20000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.863797</td>\n",
       "      <td>28.662250</td>\n",
       "      <td>5.422928</td>\n",
       "      <td>1.096130</td>\n",
       "      <td>1424.403050</td>\n",
       "      <td>3.074532</td>\n",
       "      <td>35.627571</td>\n",
       "      <td>-119.565517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.890949</td>\n",
       "      <td>12.582213</td>\n",
       "      <td>2.460834</td>\n",
       "      <td>0.466714</td>\n",
       "      <td>1127.754759</td>\n",
       "      <td>10.549621</td>\n",
       "      <td>2.135219</td>\n",
       "      <td>2.003148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.499900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>32.540000</td>\n",
       "      <td>-124.350000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.562500</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>4.438356</td>\n",
       "      <td>1.005913</td>\n",
       "      <td>788.000000</td>\n",
       "      <td>2.428571</td>\n",
       "      <td>33.930000</td>\n",
       "      <td>-121.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.531750</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>5.225898</td>\n",
       "      <td>1.048780</td>\n",
       "      <td>1166.000000</td>\n",
       "      <td>2.817544</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>-118.490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.736250</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>6.048504</td>\n",
       "      <td>1.099885</td>\n",
       "      <td>1724.000000</td>\n",
       "      <td>3.281612</td>\n",
       "      <td>37.710000</td>\n",
       "      <td>-118.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.000100</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>141.909091</td>\n",
       "      <td>34.066667</td>\n",
       "      <td>35682.000000</td>\n",
       "      <td>1243.333333</td>\n",
       "      <td>41.950000</td>\n",
       "      <td>-114.310000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\n",
       "count  20000.000000  20000.000000  20000.000000  20000.000000  20000.000000   \n",
       "mean       3.863797     28.662250      5.422928      1.096130   1424.403050   \n",
       "std        1.890949     12.582213      2.460834      0.466714   1127.754759   \n",
       "min        0.499900      1.000000      0.846154      0.333333      3.000000   \n",
       "25%        2.562500     18.000000      4.438356      1.005913    788.000000   \n",
       "50%        3.531750     29.000000      5.225898      1.048780   1166.000000   \n",
       "75%        4.736250     37.000000      6.048504      1.099885   1724.000000   \n",
       "max       15.000100     52.000000    141.909091     34.066667  35682.000000   \n",
       "\n",
       "           AveOccup      Latitude     Longitude  \n",
       "count  20000.000000  20000.000000  20000.000000  \n",
       "mean       3.074532     35.627571   -119.565517  \n",
       "std       10.549621      2.135219      2.003148  \n",
       "min        0.692308     32.540000   -124.350000  \n",
       "25%        2.428571     33.930000   -121.790000  \n",
       "50%        2.817544     34.250000   -118.490000  \n",
       "75%        3.281612     37.710000   -118.010000  \n",
       "max     1243.333333     41.950000   -114.310000  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's have a look at the data. Is it in similar ranges?\n",
    "import pandas as pd\n",
    "pd.DataFrame(train_data, columns=calif.feature_names).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data comes in many different ranges: compare the ranges of latitude (32.54 to 41.95), longitude (-124.35 to -114.31), median income (0.50 to 15.0) and population (3 to 35682). As we described earlier, it's a good idea to normalize these values. Scikit-Learn has several built-in scalers that do just this. We will use the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html), which changes the data so it is normally distributed, with a mean of 0 and a variance of 1, but other [standardization methods are available](https://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2.000000e+04</td>\n",
       "      <td>2.000000e+04</td>\n",
       "      <td>2.000000e+04</td>\n",
       "      <td>2.000000e+04</td>\n",
       "      <td>2.000000e+04</td>\n",
       "      <td>2.000000e+04</td>\n",
       "      <td>2.000000e+04</td>\n",
       "      <td>2.000000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-5.421441e-15</td>\n",
       "      <td>-1.563194e-17</td>\n",
       "      <td>-1.909051e-15</td>\n",
       "      <td>1.616378e-14</td>\n",
       "      <td>9.308110e-17</td>\n",
       "      <td>-1.283240e-15</td>\n",
       "      <td>-8.529986e-14</td>\n",
       "      <td>-6.669655e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000025e+00</td>\n",
       "      <td>1.000025e+00</td>\n",
       "      <td>1.000025e+00</td>\n",
       "      <td>1.000025e+00</td>\n",
       "      <td>1.000025e+00</td>\n",
       "      <td>1.000025e+00</td>\n",
       "      <td>1.000025e+00</td>\n",
       "      <td>1.000025e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.778991e+00</td>\n",
       "      <td>-2.198575e+00</td>\n",
       "      <td>-1.859893e+00</td>\n",
       "      <td>-1.634440e+00</td>\n",
       "      <td>-1.260415e+00</td>\n",
       "      <td>-2.258170e-01</td>\n",
       "      <td>-1.446057e+00</td>\n",
       "      <td>-2.388542e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.881886e-01</td>\n",
       "      <td>-8.474277e-01</td>\n",
       "      <td>-4.001068e-01</td>\n",
       "      <td>-1.933085e-01</td>\n",
       "      <td>-5.643239e-01</td>\n",
       "      <td>-6.123219e-02</td>\n",
       "      <td>-7.950534e-01</td>\n",
       "      <td>-1.110522e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-1.756024e-01</td>\n",
       "      <td>2.684412e-02</td>\n",
       "      <td>-8.006845e-02</td>\n",
       "      <td>-1.014562e-01</td>\n",
       "      <td>-2.291363e-01</td>\n",
       "      <td>-2.436055e-02</td>\n",
       "      <td>-6.451821e-01</td>\n",
       "      <td>5.369269e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.613953e-01</td>\n",
       "      <td>6.626782e-01</td>\n",
       "      <td>2.542194e-01</td>\n",
       "      <td>8.044084e-03</td>\n",
       "      <td>2.656645e-01</td>\n",
       "      <td>1.962963e-02</td>\n",
       "      <td>9.753013e-01</td>\n",
       "      <td>7.765558e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.889415e+00</td>\n",
       "      <td>1.854867e+00</td>\n",
       "      <td>5.546477e+01</td>\n",
       "      <td>7.064575e+01</td>\n",
       "      <td>3.037757e+01</td>\n",
       "      <td>1.175672e+02</td>\n",
       "      <td>2.961096e+00</td>\n",
       "      <td>2.623695e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MedInc      HouseAge      AveRooms     AveBedrms    Population  \\\n",
       "count  2.000000e+04  2.000000e+04  2.000000e+04  2.000000e+04  2.000000e+04   \n",
       "mean  -5.421441e-15 -1.563194e-17 -1.909051e-15  1.616378e-14  9.308110e-17   \n",
       "std    1.000025e+00  1.000025e+00  1.000025e+00  1.000025e+00  1.000025e+00   \n",
       "min   -1.778991e+00 -2.198575e+00 -1.859893e+00 -1.634440e+00 -1.260415e+00   \n",
       "25%   -6.881886e-01 -8.474277e-01 -4.001068e-01 -1.933085e-01 -5.643239e-01   \n",
       "50%   -1.756024e-01  2.684412e-02 -8.006845e-02 -1.014562e-01 -2.291363e-01   \n",
       "75%    4.613953e-01  6.626782e-01  2.542194e-01  8.044084e-03  2.656645e-01   \n",
       "max    5.889415e+00  1.854867e+00  5.546477e+01  7.064575e+01  3.037757e+01   \n",
       "\n",
       "           AveOccup      Latitude     Longitude  \n",
       "count  2.000000e+04  2.000000e+04  2.000000e+04  \n",
       "mean  -1.283240e-15 -8.529986e-14 -6.669655e-13  \n",
       "std    1.000025e+00  1.000025e+00  1.000025e+00  \n",
       "min   -2.258170e-01 -1.446057e+00 -2.388542e+00  \n",
       "25%   -6.123219e-02 -7.950534e-01 -1.110522e+00  \n",
       "50%   -2.436055e-02 -6.451821e-01  5.369269e-01  \n",
       "75%    1.962963e-02  9.753013e-01  7.765558e-01  \n",
       "max    1.175672e+02  2.961096e+00  2.623695e+00  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Figure out how to scale all the input features in the training dataset.\n",
    "scaler.fit(train_data)\n",
    "scaled_train_data = scaler.transform(train_data)\n",
    "\n",
    "# Also tranform our validation and testing data in the same way.\n",
    "scaled_test_data = scaler.transform(test_data)\n",
    "\n",
    "# Did that work?\n",
    "pd.DataFrame(scaled_train_data, columns=calif.feature_names).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, we're ready to run our model! We will use the [MLPRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html) -- MLP stands for [multilevel perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron), a description of this kind of neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.98128534\n",
      "Validation score: 0.419891\n",
      "Iteration 2, loss = 0.33635107\n",
      "Validation score: 0.568898\n",
      "Iteration 3, loss = 0.25663779\n",
      "Validation score: 0.642041\n",
      "Iteration 4, loss = 0.22362402\n",
      "Validation score: 0.671661\n",
      "Iteration 5, loss = 0.20866553\n",
      "Validation score: 0.688532\n",
      "Iteration 6, loss = 0.19952123\n",
      "Validation score: 0.705023\n",
      "Iteration 7, loss = 0.19196451\n",
      "Validation score: 0.716559\n",
      "Iteration 8, loss = 0.18646636\n",
      "Validation score: 0.724928\n",
      "Iteration 9, loss = 0.18221476\n",
      "Validation score: 0.730734\n",
      "Iteration 10, loss = 0.17907250\n",
      "Validation score: 0.734858\n",
      "Iteration 11, loss = 0.17538714\n",
      "Validation score: 0.740256\n",
      "Iteration 12, loss = 0.17181941\n",
      "Validation score: 0.743008\n",
      "Iteration 13, loss = 0.16921575\n",
      "Validation score: 0.748105\n",
      "Iteration 14, loss = 0.16624208\n",
      "Validation score: 0.749254\n",
      "Iteration 15, loss = 0.16344204\n",
      "Validation score: 0.751968\n",
      "Iteration 16, loss = 0.16113209\n",
      "Validation score: 0.757984\n",
      "Iteration 17, loss = 0.15892381\n",
      "Validation score: 0.758862\n",
      "Iteration 18, loss = 0.15666842\n",
      "Validation score: 0.760867\n",
      "Iteration 19, loss = 0.15535591\n",
      "Validation score: 0.762115\n",
      "Iteration 20, loss = 0.15641656\n",
      "Validation score: 0.759568\n",
      "Iteration 21, loss = 0.15363270\n",
      "Validation score: 0.764615\n",
      "Iteration 22, loss = 0.15281970\n",
      "Validation score: 0.768114\n",
      "Iteration 23, loss = 0.15235991\n",
      "Validation score: 0.763345\n",
      "Iteration 24, loss = 0.15169789\n",
      "Validation score: 0.767620\n",
      "Iteration 25, loss = 0.15169852\n",
      "Validation score: 0.764797\n",
      "Iteration 26, loss = 0.14882247\n",
      "Validation score: 0.773178\n",
      "Iteration 27, loss = 0.14730356\n",
      "Validation score: 0.768843\n",
      "Iteration 28, loss = 0.14744194\n",
      "Validation score: 0.774659\n",
      "Iteration 29, loss = 0.14737836\n",
      "Validation score: 0.776062\n",
      "Iteration 30, loss = 0.14540097\n",
      "Validation score: 0.775790\n",
      "Iteration 31, loss = 0.14678014\n",
      "Validation score: 0.775398\n",
      "Iteration 32, loss = 0.15213976\n",
      "Validation score: 0.777790\n",
      "Iteration 33, loss = 0.14450709\n",
      "Validation score: 0.777304\n",
      "Iteration 34, loss = 0.14349856\n",
      "Validation score: 0.778552\n",
      "Iteration 35, loss = 0.14357113\n",
      "Validation score: 0.780236\n",
      "Iteration 36, loss = 0.14256278\n",
      "Validation score: 0.776341\n",
      "Iteration 37, loss = 0.14157218\n",
      "Validation score: 0.779281\n",
      "Iteration 38, loss = 0.14178539\n",
      "Validation score: 0.771302\n",
      "Iteration 39, loss = 0.14175722\n",
      "Validation score: 0.781482\n",
      "Iteration 40, loss = 0.14014429\n",
      "Validation score: 0.779777\n",
      "Iteration 41, loss = 0.14398973\n",
      "Validation score: 0.767444\n",
      "Iteration 42, loss = 0.14051056\n",
      "Validation score: 0.782649\n",
      "Iteration 43, loss = 0.14008452\n",
      "Validation score: 0.776862\n",
      "Iteration 44, loss = 0.14222616\n",
      "Validation score: 0.779174\n",
      "Iteration 45, loss = 0.14042190\n",
      "Validation score: 0.782921\n",
      "Iteration 46, loss = 0.13794420\n",
      "Validation score: 0.781404\n",
      "Iteration 47, loss = 0.13808171\n",
      "Validation score: 0.782298\n",
      "Iteration 48, loss = 0.13792605\n",
      "Validation score: 0.786198\n",
      "Iteration 49, loss = 0.13782301\n",
      "Validation score: 0.775951\n",
      "Iteration 50, loss = 0.13748369\n",
      "Validation score: 0.785547\n",
      "Iteration 51, loss = 0.13637831\n",
      "Validation score: 0.778668\n",
      "Iteration 52, loss = 0.13670431\n",
      "Validation score: 0.781787\n",
      "Iteration 53, loss = 0.13604419\n",
      "Validation score: 0.784449\n",
      "Iteration 54, loss = 0.13626378\n",
      "Validation score: 0.786217\n",
      "Iteration 55, loss = 0.13515232\n",
      "Validation score: 0.783131\n",
      "Iteration 56, loss = 0.13595276\n",
      "Validation score: 0.786386\n",
      "Iteration 57, loss = 0.13501632\n",
      "Validation score: 0.787069\n",
      "Iteration 58, loss = 0.13492809\n",
      "Validation score: 0.784663\n",
      "Iteration 59, loss = 0.13585218\n",
      "Validation score: 0.783906\n",
      "Iteration 60, loss = 0.13506649\n",
      "Validation score: 0.785370\n",
      "Iteration 61, loss = 0.13415254\n",
      "Validation score: 0.788953\n",
      "Iteration 62, loss = 0.13443373\n",
      "Validation score: 0.788409\n",
      "Iteration 63, loss = 0.13365842\n",
      "Validation score: 0.786703\n",
      "Iteration 64, loss = 0.13455042\n",
      "Validation score: 0.788045\n",
      "Iteration 65, loss = 0.13423940\n",
      "Validation score: 0.789248\n",
      "Iteration 66, loss = 0.13361084\n",
      "Validation score: 0.786147\n",
      "Iteration 67, loss = 0.13442412\n",
      "Validation score: 0.783363\n",
      "Iteration 68, loss = 0.13327358\n",
      "Validation score: 0.787573\n",
      "Iteration 69, loss = 0.13371211\n",
      "Validation score: 0.782753\n",
      "Iteration 70, loss = 0.13172710\n",
      "Validation score: 0.788406\n",
      "Iteration 71, loss = 0.13137831\n",
      "Validation score: 0.788969\n",
      "Iteration 72, loss = 0.13102463\n",
      "Validation score: 0.791692\n",
      "Iteration 73, loss = 0.13147145\n",
      "Validation score: 0.787432\n",
      "Iteration 74, loss = 0.13077310\n",
      "Validation score: 0.788266\n",
      "Iteration 75, loss = 0.13156883\n",
      "Validation score: 0.790373\n",
      "Iteration 76, loss = 0.13026564\n",
      "Validation score: 0.786956\n",
      "Iteration 77, loss = 0.13092590\n",
      "Validation score: 0.782865\n",
      "Iteration 78, loss = 0.13197925\n",
      "Validation score: 0.788875\n",
      "Iteration 79, loss = 0.12986272\n",
      "Validation score: 0.789196\n",
      "Iteration 80, loss = 0.13040346\n",
      "Validation score: 0.793397\n",
      "Iteration 81, loss = 0.13009348\n",
      "Validation score: 0.788910\n",
      "Iteration 82, loss = 0.12973886\n",
      "Validation score: 0.791032\n",
      "Iteration 83, loss = 0.13018742\n",
      "Validation score: 0.791706\n",
      "Iteration 84, loss = 0.12874194\n",
      "Validation score: 0.791743\n",
      "Iteration 85, loss = 0.12835030\n",
      "Validation score: 0.792257\n",
      "Iteration 86, loss = 0.12855830\n",
      "Validation score: 0.790647\n",
      "Iteration 87, loss = 0.12938745\n",
      "Validation score: 0.791212\n",
      "Iteration 88, loss = 0.12791124\n",
      "Validation score: 0.791302\n",
      "Iteration 89, loss = 0.12786675\n",
      "Validation score: 0.786974\n",
      "Iteration 90, loss = 0.12769156\n",
      "Validation score: 0.790724\n",
      "Iteration 91, loss = 0.12721867\n",
      "Validation score: 0.790747\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPRegressor(activation='relu', alpha=0.001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 20), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "ann = MLPRegressor(\n",
    "    activation='relu',  # The activation function to use: ReLU\n",
    "    solver='adam',      # The solver to use\n",
    "    alpha=0.001,        # The L2 regularization rate: higher values increase cost for larger weights\n",
    "    hidden_layer_sizes=(100, 20),\n",
    "                        # The number of units in each hidden layer.\n",
    "                        # Note that we don't need to specify input and output neuron numbers:\n",
    "                        # MLPClassifier determines this based on the shape of the features and labels\n",
    "                        # being fitted.\n",
    "    verbose=True,       # Report on progress.\n",
    "    batch_size='auto',  # Process dataset in batches of 200 rows at a time.\n",
    "    early_stopping=True # This activates two features:\n",
    "                        #  - We will hold 10% of data aside as validation data. At the end of each\n",
    "                        #    iteration, we will test the validation data to see how well we're doing.\n",
    "                        #  - If learning slows below a pre-determined level, we stop early rather than\n",
    "                        #    overtraining on our data.\n",
    ")\n",
    "ann.fit(scaled_train_data, train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three things to note:\n",
    " - Some models will never converge, in which case you will see a warning. This suggests that learning is *not* completely, and is likely because something is wrong with learning using this dataset with these hyperparameters.\n",
    " - We learn iteratively. In Scikit-Learn, each iteration is further broken up into \"batches\" of data.\n",
    " - We expect to see loss going down over time and validation score going up over time. We can visualize these in a graph if we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the loss curve and validation scores over iterations.\n",
    "plt.plot(ann.loss_curve_, label='Loss at iteration')\n",
    "plt.plot(ann.validation_scores_, label='Validation scores at iteration')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can use our test data to check how our model performs on data that it has not been previously exposed to. Let's see how we did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8134489126715619"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann.score(scaled_test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "81.3% accuracy! Not bad, but it could definitely be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Backpropagation\n",
    "\n",
    "The heart of neural networks is [backpropagation algorithms](https://en.wikipedia.org/wiki/Backpropagation), which are an efficient way to change the weights and biases in the ANN based on the size of the loss.\n",
    "\n",
    "In effect, an ANN is trained by:\n",
    "1. Setting all weights and biases randomly.\n",
    "2. For each row in the test data:\n",
    "    1. Set the input units to the input features.\n",
    "    2. Use unit weights and biases, passing through the activation function, to calculate the output value of each unit -- right through to the output units.\n",
    "    3. Use a loss function to compare the output units with the expected output.\n",
    "    4. Use a backpropagation algorithm to update all the weights and biases to reduce the loss.\n",
    "\n",
    "Google has a [nice visual explanation](https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/) of backpropagation. [More detailed explanations](http://neuralnetworksanddeeplearning.com/chap2.html) are also available.\n",
    "    \n",
    "## When backpropagation goes wrong\n",
    "\n",
    "* Vanishing gradients: when weights for lower levels (closer to the input) become very small, gradients become very small too, making it hard or impossible to train these layers. The ReLU activation function can help prevent vanishing gradients.\n",
    "\n",
    "* Exploding gradients: when weights become very large, the gradients for lower layers can become very large, making it hard for these gradients to converge. Batch normalization can help prevent exploding gradients, as can lowering the learning rate.\n",
    "\n",
    "* Dead ReLU units: once the weighted sum for a ReLU activation function falls below 0, the ReLU unit can get stuck -- without an output, it doesn't contribute to the network output, and gradients can't flow through it in backpropagation. Lowering the learning rate can help keep ReLU units from dying.\n",
    "\n",
    "* Dropout regularization: in this form of regularization, a proportion of unit activations are randomly dropped out. This prevents overfitting and so helps create a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## ANN for classification: what sort of forest is this?\n",
    "Let's jump in with a dataset called [Covertype](https://archive.ics.uci.edu/ml/datasets/Covertype), where we try to predict forest cover type based on a number of features of a 30x30m area of forest as follows:\n",
    "\n",
    "| Column | Feature | Units | Description | How measured |\n",
    "|---|--------|-------|-------------|--------------|\n",
    "| 1 | Aspect | degrees azimuth | Aspect in degrees azimuth | Quantitative |\n",
    "| 2 | Slope | degrees | Slope in degrees | Quantitative |\n",
    "| 3 | Horizontal_Distance_To_Hydrology | meters | Horz Dist to nearest surface water features | Quantitative |\n",
    "| 4 | Vertical_Distance_To_Hydrology | meters | Vert Dist to nearest surface water features | Quantitative |\n",
    "| 5 | Horizontal_Distance_To_Roadways | meters | Horz Dist to nearest roadway | Quantitative |\n",
    "| 6 | Hillshade_9am | 0 to 255 index | Hillshade index at 9am, summer solstice | Quantitative |\n",
    "| 7 | Hillshade_Noon | 0 to 255 index | Hillshade index at noon, summer soltice | Quantitative |\n",
    "| 8 | Hillshade_3pm | 0 to 255 index | Hillshade index at 3pm, summer solstice | Quantitative |\n",
    "| 9 | Horizontal_Distance_To_Fire_Points | meters | Horz Dist to nearest wildfire ignition points | Quantitative |\n",
    "| 10-14 | Wilderness_Area | 4 binary columns with 0 (absence) or 1 (presence) | Which wilderness area this plot is in | Qualitative |\n",
    "| 14-54 | Soil_Type | 40 binary columns with 0 (absence) or 1 (presence) | Soil Type designation | Qualitative |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this information, we are trying to classify each 30x30m plot as one of seven forest types.\n",
    "\n",
    "This dataset is built into Scikit, so we can use it to download and load the dataset for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fetch_covtype in module sklearn.datasets.covtype:\n",
      "\n",
      "fetch_covtype(data_home=None, download_if_missing=True, random_state=None, shuffle=False, return_X_y=False)\n",
      "    Load the covertype dataset (classification).\n",
      "    \n",
      "    Download it if necessary.\n",
      "    \n",
      "    =================   ============\n",
      "    Classes                        7\n",
      "    Samples total             581012\n",
      "    Dimensionality                54\n",
      "    Features                     int\n",
      "    =================   ============\n",
      "    \n",
      "    Read more in the :ref:`User Guide <covtype_dataset>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    data_home : string, optional\n",
      "        Specify another download and cache folder for the datasets. By default\n",
      "        all scikit-learn data is stored in '~/scikit_learn_data' subfolders.\n",
      "    \n",
      "    download_if_missing : boolean, default=True\n",
      "        If False, raise a IOError if the data is not locally available\n",
      "        instead of trying to download the data from the source site.\n",
      "    \n",
      "    random_state : int, RandomState instance or None (default)\n",
      "        Determines random number generation for dataset shuffling. Pass an int\n",
      "        for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "    \n",
      "    shuffle : bool, default=False\n",
      "        Whether to shuffle dataset.\n",
      "    \n",
      "    return_X_y : boolean, default=False.\n",
      "        If True, returns ``(data.data, data.target)`` instead of a Bunch\n",
      "        object.\n",
      "    \n",
      "        .. versionadded:: 0.20\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    dataset : dict-like object with the following attributes:\n",
      "    \n",
      "    dataset.data : numpy array of shape (581012, 54)\n",
      "        Each row corresponds to the 54 features in the dataset.\n",
      "    \n",
      "    dataset.target : numpy array of shape (581012,)\n",
      "        Each value corresponds to one of the 7 forest covertypes with values\n",
      "        ranging between 1 to 7.\n",
      "    \n",
      "    dataset.DESCR : string\n",
      "        Description of the forest covertype dataset.\n",
      "    \n",
      "    (data, target) : tuple if ``return_X_y`` is True\n",
      "    \n",
      "        .. versionadded:: 0.20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "help(datasets.fetch_covtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we don't need to provide any arguments, but it warns us that it will need to download this dataset. It also describes the the returned dataset object will have the following properties:\n",
    "- .data: a numpy array with the features.\n",
    "- .target: a numpy array with the target labels. Note that each plot is classified into only one of these values.\n",
    "- .DESCR: describe this forest covertype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data:  [[2.771e+03 1.690e+02 1.000e+01 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.940e+03 3.350e+02 2.900e+01 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.692e+03 0.000e+00 2.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " ...\n",
      " [2.601e+03 5.400e+01 1.100e+01 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.679e+03 6.000e+01 1.100e+01 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.880e+03 1.530e+02 9.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]]\n",
      "Data shape:  (581012, 54)\n"
     ]
    }
   ],
   "source": [
    "# Let's get the data pre-shuffled.\n",
    "covtype = datasets.fetch_covtype(shuffle=True)\n",
    "\n",
    "print(\"Data: \", covtype.data)\n",
    "print(\"Data shape: \", covtype.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target:  [2 2 2 ... 2 2 2]\n",
      "Target shape:  (581012,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Target: \", covtype.target)\n",
    "print(\"Target shape: \", covtype.target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we start by splitting our data into test and training data. We'll set aside 50,000 rows for testing, and use the remaining 531,012 rows for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data shape:  (50000, 54)\n",
      "Test labels shape:  (50000,)\n",
      "Training data shape:  (531012, 54)\n",
      "Training labels shape:  (531012,)\n"
     ]
    }
   ],
   "source": [
    "# Out of 581,012 data entries, let's hold back:\n",
    "# - 50,000 records as our test dataset\n",
    "# - remaining records as our training dataset\n",
    "\n",
    "test_data = covtype.data[0:50_000]\n",
    "test_labels = covtype.target[0:50_000]\n",
    "\n",
    "train_data = covtype.data[50_000:]\n",
    "train_labels = covtype.target[50_000:]\n",
    "\n",
    "print(\"Test data shape: \", test_data.shape)\n",
    "print(\"Test labels shape: \", test_labels.shape)\n",
    "\n",
    "print(\"Training data shape: \", train_data.shape)\n",
    "print(\"Training labels shape: \", train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is ready for processing. But remember that we have a variety of different input types: binary (0, 1), continuous in small ranges (0-255) and in large ranges (elevations). Before we process this data, we should normalize them into a standard range. We'll use a MinMaxScaler: it reduces the range of all data so they fit into the range 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>531012.000000</td>\n",
       "      <td>531012.000000</td>\n",
       "      <td>531012.000000</td>\n",
       "      <td>531012.000000</td>\n",
       "      <td>531012.000000</td>\n",
       "      <td>531012.000000</td>\n",
       "      <td>531012.000000</td>\n",
       "      <td>531012.000000</td>\n",
       "      <td>531012.000000</td>\n",
       "      <td>531012.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>531012.000000</td>\n",
       "      <td>531012.000000</td>\n",
       "      <td>531012.000000</td>\n",
       "      <td>531012.000000</td>\n",
       "      <td>531012.000000</td>\n",
       "      <td>531012.000000</td>\n",
       "      <td>531012.000000</td>\n",
       "      <td>531012.000000</td>\n",
       "      <td>531012.000000</td>\n",
       "      <td>531012.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.550429</td>\n",
       "      <td>0.432471</td>\n",
       "      <td>0.213629</td>\n",
       "      <td>0.192742</td>\n",
       "      <td>0.283429</td>\n",
       "      <td>0.330305</td>\n",
       "      <td>0.835211</td>\n",
       "      <td>0.879196</td>\n",
       "      <td>0.561152</td>\n",
       "      <td>0.276004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044148</td>\n",
       "      <td>0.090486</td>\n",
       "      <td>0.077622</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>0.003262</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.026804</td>\n",
       "      <td>0.023747</td>\n",
       "      <td>0.015000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.140044</td>\n",
       "      <td>0.310960</td>\n",
       "      <td>0.113477</td>\n",
       "      <td>0.152022</td>\n",
       "      <td>0.075287</td>\n",
       "      <td>0.219127</td>\n",
       "      <td>0.105353</td>\n",
       "      <td>0.077829</td>\n",
       "      <td>0.150669</td>\n",
       "      <td>0.184460</td>\n",
       "      <td>...</td>\n",
       "      <td>0.205424</td>\n",
       "      <td>0.286877</td>\n",
       "      <td>0.267575</td>\n",
       "      <td>0.052506</td>\n",
       "      <td>0.057018</td>\n",
       "      <td>0.014457</td>\n",
       "      <td>0.022627</td>\n",
       "      <td>0.161509</td>\n",
       "      <td>0.152260</td>\n",
       "      <td>0.121551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.475238</td>\n",
       "      <td>0.161111</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.077309</td>\n",
       "      <td>0.232558</td>\n",
       "      <td>0.155403</td>\n",
       "      <td>0.779528</td>\n",
       "      <td>0.838583</td>\n",
       "      <td>0.468504</td>\n",
       "      <td>0.142758</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.568784</td>\n",
       "      <td>0.352778</td>\n",
       "      <td>0.196970</td>\n",
       "      <td>0.156049</td>\n",
       "      <td>0.260982</td>\n",
       "      <td>0.280736</td>\n",
       "      <td>0.858268</td>\n",
       "      <td>0.889764</td>\n",
       "      <td>0.562992</td>\n",
       "      <td>0.238394</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.652326</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.274875</td>\n",
       "      <td>0.312661</td>\n",
       "      <td>0.467894</td>\n",
       "      <td>0.909449</td>\n",
       "      <td>0.933071</td>\n",
       "      <td>0.661417</td>\n",
       "      <td>0.355500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows  54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0              1              2              3   \\\n",
       "count  531012.000000  531012.000000  531012.000000  531012.000000   \n",
       "mean        0.550429       0.432471       0.213629       0.192742   \n",
       "std         0.140044       0.310960       0.113477       0.152022   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.475238       0.161111       0.136364       0.077309   \n",
       "50%         0.568784       0.352778       0.196970       0.156049   \n",
       "75%         0.652326       0.725000       0.272727       0.274875   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                  4              5              6              7   \\\n",
       "count  531012.000000  531012.000000  531012.000000  531012.000000   \n",
       "mean        0.283429       0.330305       0.835211       0.879196   \n",
       "std         0.075287       0.219127       0.105353       0.077829   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.232558       0.155403       0.779528       0.838583   \n",
       "50%         0.260982       0.280736       0.858268       0.889764   \n",
       "75%         0.312661       0.467894       0.909449       0.933071   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                  8              9       ...                   44  \\\n",
       "count  531012.000000  531012.000000      ...        531012.000000   \n",
       "mean        0.561152       0.276004      ...             0.044148   \n",
       "std         0.150669       0.184460      ...             0.205424   \n",
       "min         0.000000       0.000000      ...             0.000000   \n",
       "25%         0.468504       0.142758      ...             0.000000   \n",
       "50%         0.562992       0.238394      ...             0.000000   \n",
       "75%         0.661417       0.355500      ...             0.000000   \n",
       "max         1.000000       1.000000      ...             1.000000   \n",
       "\n",
       "                  45             46             47             48  \\\n",
       "count  531012.000000  531012.000000  531012.000000  531012.000000   \n",
       "mean        0.090486       0.077622       0.002765       0.003262   \n",
       "std         0.286877       0.267575       0.052506       0.057018   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                  49             50             51             52  \\\n",
       "count  531012.000000  531012.000000  531012.000000  531012.000000   \n",
       "mean        0.000209       0.000512       0.026804       0.023747   \n",
       "std         0.014457       0.022627       0.161509       0.152260   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "                  53  \n",
       "count  531012.000000  \n",
       "mean        0.015000  \n",
       "std         0.121551  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 54 columns]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Figure out how to scale all the input features in the training dataset.\n",
    "scaler.fit(train_data)\n",
    "scaled_train_data = scaler.transform(train_data)\n",
    "\n",
    "# Also tranform our validation and testing data in the same way.\n",
    "scaled_test_data = scaler.transform(test_data)\n",
    "\n",
    "# Did that work?\n",
    "pd.DataFrame(scaled_train_data).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying among multiple categories\n",
    "\n",
    "Having multiple output units usually would result in each unit being considered independently, allowing you to assign multiple labels for a particular input (for instance, a single image might be classified as containing both a cloud as well as a bird). However, we use scikit-learn's [MLPClassifier](https://scikit-learn.org/stable/modules/neural_networks_supervised.html#classification), which automatically uses *softmax* to treat labels as exclusive to each other.\n",
    "\n",
    "### The power of softmax\n",
    "\n",
    "[Softmax](https://developers.google.com/machine-learning/crash-course/multi-class-neural-networks/softmax) is an activation function added to the output units that ensures that the sum of the outputs of all units in the output layer is 100%. The output of each individual unit is therefore the probability that it is the category into which the input should be categorized.\n",
    "\n",
    "The result of this is that MLPClassifier can provide both a predicted label for a set of input features, as well as the probability that represents how certain the model is about this prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.68623661\n",
      "Validation score: 0.733456\n",
      "Iteration 2, loss = 0.58782608\n",
      "Validation score: 0.754642\n",
      "Iteration 3, loss = 0.55841590\n",
      "Validation score: 0.757052\n",
      "Iteration 4, loss = 0.54114955\n",
      "Validation score: 0.768916\n",
      "Iteration 5, loss = 0.52823728\n",
      "Validation score: 0.773606\n",
      "Iteration 6, loss = 0.51759235\n",
      "Validation score: 0.768860\n",
      "Iteration 7, loss = 0.50863469\n",
      "Validation score: 0.779180\n",
      "Iteration 8, loss = 0.50075337\n",
      "Validation score: 0.786204\n",
      "Iteration 9, loss = 0.49368298\n",
      "Validation score: 0.789895\n",
      "Iteration 10, loss = 0.48612726\n",
      "Validation score: 0.792720\n",
      "Iteration 11, loss = 0.47997282\n",
      "Validation score: 0.798275\n",
      "Iteration 12, loss = 0.47329193\n",
      "Validation score: 0.797993\n",
      "Iteration 13, loss = 0.46752103\n",
      "Validation score: 0.800328\n",
      "Iteration 14, loss = 0.46225088\n",
      "Validation score: 0.802192\n",
      "Iteration 15, loss = 0.45795770\n",
      "Validation score: 0.804282\n",
      "Iteration 16, loss = 0.45324666\n",
      "Validation score: 0.795639\n",
      "Iteration 17, loss = 0.44996015\n",
      "Validation score: 0.808162\n",
      "Iteration 18, loss = 0.44642140\n",
      "Validation score: 0.809555\n",
      "Iteration 19, loss = 0.44228636\n",
      "Validation score: 0.815035\n",
      "Iteration 20, loss = 0.43874381\n",
      "Validation score: 0.813133\n",
      "Iteration 21, loss = 0.43535125\n",
      "Validation score: 0.813548\n",
      "Iteration 22, loss = 0.43290838\n",
      "Validation score: 0.811909\n",
      "Iteration 23, loss = 0.42912131\n",
      "Validation score: 0.816052\n",
      "Iteration 24, loss = 0.42687433\n",
      "Validation score: 0.820760\n",
      "Iteration 25, loss = 0.42435295\n",
      "Validation score: 0.822172\n",
      "Iteration 26, loss = 0.42224263\n",
      "Validation score: 0.821928\n",
      "Iteration 27, loss = 0.41946037\n",
      "Validation score: 0.825317\n",
      "Iteration 28, loss = 0.41748071\n",
      "Validation score: 0.824206\n",
      "Iteration 29, loss = 0.41600533\n",
      "Validation score: 0.825393\n",
      "Iteration 30, loss = 0.41453001\n",
      "Validation score: 0.818293\n",
      "Iteration 31, loss = 0.41171175\n",
      "Validation score: 0.822624\n",
      "Iteration 32, loss = 0.41049822\n",
      "Validation score: 0.826579\n",
      "Iteration 33, loss = 0.40892801\n",
      "Validation score: 0.817540\n",
      "Iteration 34, loss = 0.40691610\n",
      "Validation score: 0.824169\n",
      "Iteration 35, loss = 0.40585286\n",
      "Validation score: 0.830157\n",
      "Iteration 36, loss = 0.40453388\n",
      "Validation score: 0.829178\n",
      "Iteration 37, loss = 0.40313583\n",
      "Validation score: 0.828782\n",
      "Iteration 38, loss = 0.40152173\n",
      "Validation score: 0.830119\n",
      "Iteration 39, loss = 0.40091026\n",
      "Validation score: 0.834055\n",
      "Iteration 40, loss = 0.39842218\n",
      "Validation score: 0.829310\n",
      "Iteration 41, loss = 0.39731666\n",
      "Validation score: 0.824263\n",
      "Iteration 42, loss = 0.39656999\n",
      "Validation score: 0.829159\n",
      "Iteration 43, loss = 0.39540370\n",
      "Validation score: 0.832549\n",
      "Iteration 44, loss = 0.39426514\n",
      "Validation score: 0.833829\n",
      "Iteration 45, loss = 0.39245349\n",
      "Validation score: 0.837031\n",
      "Iteration 46, loss = 0.39196859\n",
      "Validation score: 0.833001\n",
      "Iteration 47, loss = 0.39033366\n",
      "Validation score: 0.838518\n",
      "Iteration 48, loss = 0.38992305\n",
      "Validation score: 0.833076\n",
      "Iteration 49, loss = 0.38892594\n",
      "Validation score: 0.838725\n",
      "Iteration 50, loss = 0.38787233\n",
      "Validation score: 0.834564\n",
      "Iteration 51, loss = 0.38767934\n",
      "Validation score: 0.839780\n",
      "Iteration 52, loss = 0.38544036\n",
      "Validation score: 0.833509\n",
      "Iteration 53, loss = 0.38505581\n",
      "Validation score: 0.833415\n",
      "Iteration 54, loss = 0.38432878\n",
      "Validation score: 0.834677\n",
      "Iteration 55, loss = 0.38301213\n",
      "Validation score: 0.834526\n",
      "Iteration 56, loss = 0.38293314\n",
      "Validation score: 0.836541\n",
      "Iteration 57, loss = 0.38154917\n",
      "Validation score: 0.840401\n",
      "Iteration 58, loss = 0.38154803\n",
      "Validation score: 0.838161\n",
      "Iteration 59, loss = 0.38059578\n",
      "Validation score: 0.842981\n",
      "Iteration 60, loss = 0.37980212\n",
      "Validation score: 0.832304\n",
      "Iteration 61, loss = 0.37885871\n",
      "Validation score: 0.839121\n",
      "Iteration 62, loss = 0.37733854\n",
      "Validation score: 0.842454\n",
      "Iteration 63, loss = 0.37780127\n",
      "Validation score: 0.837727\n",
      "Iteration 64, loss = 0.37707770\n",
      "Validation score: 0.843076\n",
      "Iteration 65, loss = 0.37621188\n",
      "Validation score: 0.836541\n",
      "Iteration 66, loss = 0.37563355\n",
      "Validation score: 0.841626\n",
      "Iteration 67, loss = 0.37527192\n",
      "Validation score: 0.842718\n",
      "Iteration 68, loss = 0.37441111\n",
      "Validation score: 0.836051\n",
      "Iteration 69, loss = 0.37327292\n",
      "Validation score: 0.842379\n",
      "Iteration 70, loss = 0.37248431\n",
      "Validation score: 0.841117\n",
      "Validation score did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=True, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(100, 20), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
       "       random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
       "       validation_fraction=0.1, verbose=True, warm_start=False)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classifier = MLPClassifier(\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=0.001,\n",
    "    hidden_layer_sizes=(100, 20),\n",
    "    batch_size='auto',\n",
    "    verbose=True,\n",
    "    early_stopping=True\n",
    ")\n",
    "classifier.fit(scaled_train_data, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the loss curve and validation scores over iterations.\n",
    "plt.plot(classifier.loss_curve_, label='Loss at iteration')\n",
    "plt.plot(classifier.validation_scores_, label='Validation scores at iteration')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8466"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(scaled_test_data, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's next?\n",
    "\n",
    "In the next part of this course, we will discuss the landscape of machine learning methods. Artificial neural networks are a valuable part of this landscape, and -- as you can see -- very easy to set up and try, but will not always be the best solution to the problem."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
