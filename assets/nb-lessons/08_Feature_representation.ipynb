{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The format we receive real world data in varies greatly and often needs to be modified before it can be used in machine learning models. The date must often be transformed to create a **representation** of the data which can be used.  \n",
    "\n",
    "Representing features optimally is the most important technical task in most machine learning problems. It is where practitioners of machine learning spend most of their time and it often has _the largest impact on performance_ of anything you can do.\n",
    "\n",
    "\n",
    "Data comes in many types:\n",
    " \n",
    "* real valued numbers : petal length\n",
    "* Categorical data: species name\n",
    "* constrained data: like ratios or compositional data \n",
    "\n",
    "\n",
    "## Real-world data is messy\n",
    "\n",
    " For example lets think about a classifier that identified genes involved in resisting a plant disease\n",
    " \n",
    " If the gene is involved it has a label=1. If it is not involved its label label=0.\n",
    " \n",
    " It has the following traits:\n",
    " \n",
    "feature | type  | example\n",
    "-------|--------|-------\n",
    "functional category | String | \"K00680\"\n",
    "gc_content | int between 1,0 | 0.56\n",
    "length| positive int | 1901\n",
    "identified promoter | Boolean | true\n",
    "intron length | positive int | 300\n",
    " \n",
    " ```\n",
    " 0:{functional_category: \"K00680\",\n",
    "    gc_content: 0.56,\n",
    "    length: 1901,\n",
    "    identified_promoter: True,\n",
    "    intron_length: 300\n",
    "    }\n",
    " ```\n",
    " \n",
    " ### Encoding Strings with one hot encoding\n",
    "\n",
    "In the first example the functional category essentially a level in a factor variable.  A common way to encode this in machine learning is the **_one hot encoding_**.\n",
    "\n",
    "Each unique string in a dataset is given a position in a feature vector and assigned a 1 if it is present.\n",
    "\n",
    "possible functional categories\n",
    "```\n",
    "categories = [\"K00001\", \"k00456\", \"K00680\", ...]\n",
    "example    = [    0  ,    0   ,    1   , ...]\n",
    "```\n",
    "\n",
    "The vector for this example would be:\n",
    "\n",
    " ```\n",
    " 0:{functional_category: [0,0,1],\n",
    "    gc_content: 0.56,\n",
    "    length: 1901,\n",
    "    identified_promoter: True,\n",
    "    intron_length: 300\n",
    "    }\n",
    "```\n",
    "\n",
    "A one hot vector can get really long, but don't worry, we can encode that long vector as a sparse vector that doesn't take up too much memory.\n",
    "\n",
    "### Numeric values\n",
    "\n",
    "* Numeric values both integer and real care already in a form that can be user as features in a ML model.\n",
    "\n",
    "* Be careful though, sometimes a number is really just a label. Suppose we had trimmed the \"K\" prefix off our functional category features.  Now our feature would be `00680`, an integer which corresponds to a N'acyltransferase enzyme in the KEGG gene ontology.  That has nothing to do with `00681` which is a  glutathione hydrolase enzyme.  In this case we need to use one hot encoding.\n",
    "\n",
    "** Does it mater that these numeric values are in different number spaces (real, integer, constrained? **\n",
    "\n",
    "Yes, it does.  Often data needs to be standardized or binned to make it easer to learn.\n",
    "\n",
    "### Putting it together encoding this example:\n",
    "\n",
    "```{python}\n",
    "\n",
    "0: [0.0, 0.0 ,1.0 , 0.56, 1901, 1, 300 ]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## What makes a good feature?\n",
    "\n",
    "1. It should be non-zero more than a few times. If there are not many examples of it in the training data then it isn't of much value in learning \n",
    "\n",
    "2. Represent the values in a way that makes sense to humans if possible. This make's troubleshooting easier \n",
    "\n",
    "3. Don't use magic values as flags. For example sdon't use -1 to indicate that there were no introns in the sample above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning up data \n",
    "\n",
    "\n",
    "The old adage \"Garbage in, garbage out\" very much applies to machine learning. Much of the work of the data scientist is data cleaning.\n",
    "\n",
    "Most data cleaning starts with data visualization.  IF the feature space is not too large the distribution of of each variable can show outliers. Reduced representations of data cuch as PCA can show outliers in high dimension data sets/\n",
    "\n",
    "There are a number of tools for cleaning data including\n",
    "\n",
    "* [Open Refine](http://openrefine.org/) see [this tutorial](https://datacarpentry.org/OpenRefine-ecology-lesson/00-getting-started/)\n",
    "\n",
    "\n",
    "Once outliers have been removed you need to evaluate the need for data transformation and standardization.\n",
    "\n",
    "Some menthos like Random forest classifiers (covered later) do not need data to fit a particular distribution.  other mehtods like Logistic regression work better it the data is transformed and standardized.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
