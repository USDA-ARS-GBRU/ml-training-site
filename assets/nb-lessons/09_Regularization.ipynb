{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "Authors: Brian Stucky, Carson Andorf\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Up to this point, we improved our machine learning models by minimizing training loss.  This can work very well, but there are many instances where making the training loss as small as possible can actually cause the model's performance on validation data (i.e., the validation loss) *to get worse* (see the figure below)!   This problem is referred to as overtraining or overfitting.  Overfitting occurs when the model starts to fit to characteristics of the training set that don't generalize to new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Two loss function](../nb-images/Regularization.svg)\n",
    "<div style=\"text-align: right\"> (Image from Google machine learning crash course) </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, consider this simple dataset that contains information about insect, fish, and bird species and whether or not they can fly:\n",
    "\n",
    "|Name|Class|Can fly|\n",
    "|:--:|:---:|:-----:|\n",
    "|Pileated woodpecker|Birds|Yes|\n",
    "|Emu|Birds|No|\n",
    "|Northern cardinal|Birds|Yes|\n",
    "|Blacktip shark|Cartilaginous fishes|No|\n",
    "|Bluntnose stingray|Cartilaginous fishes|No|\n",
    "|Black drum|Bony fishes|No|\n",
    "|Florida carpenter ant|Insects|No|\n",
    "|Periodical cicada|Insects|Yes|\n",
    "|Luna moth|Insects|Yes|\n",
    "\n",
    "Your task is to develop a model to classify whether or not an animal can fly, based on information available in the dataset.  Here is a relatively simple model based on these data:\n",
    "  * If the animal is a bird or an insect, predict that it can fly.\n",
    "  * Otherwise, predict that it cannot fly.\n",
    "\n",
    "This model is imperfect.  Indeed, it misclassifies 2 of the examples in the training data.  We can try developing a more complex model to reduce our training loss:\n",
    "  * If the species is a bird and has a one-word name, predict that it cannot fly.\n",
    "  * If it is a bird with a two-word name, predict that it can fly.\n",
    "  * If it is an insect with a three-word name, predict that it cannot fly.\n",
    "  * If it is an insect with a two-word name, predict that it can fly.\n",
    "  * Otherwise, predict that it cannot fly.\n",
    "\n",
    "Aha!  That model classifies each training example perfectly!\n",
    "\n",
    "When presented with new examples, however (e.g., \"albatross\" or \"zebra swallowtail butterfly\"), the more complex model will often fail spectacularly while the simpler model, although imperfect, will perform relatively well.  Clearly, our more complex model is disastrously overfitted to the training data.\n",
    "\n",
    "The bottom line is that we want our models to be general enough to work well on new examples.  Methods to help prevent overfitting are collectively referred to as *regularization* techniques.  Another way to think about regularization is that you should not trust your training examples too much.\n",
    "\n",
    "There are a variety of ways to implement regularization to prevent overfitting.  One option is to stop training early before overfitting happens. In the figure above, you would try to stop at the point on the red curve where validation loss begins to increase.  Another approach, which is often easier to implement in practice, is to explicitly penalize model complexity in the model-fitting procedure.  Instead of just minimizing loss, we minimize `loss + complexity`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. L<sub>1</sub> and L<sub>2</sub> regularization\n",
    "\n",
    "For this lesson, we will focus on two widely used regularization methods: L<sub>1</sub> and L<sub>2</sub> regularization.  Both of these methods represent model complexity as a function of the model's feature weights.\n",
    "\n",
    "Suppose we are fitting a general linear regression model, which looks like this:\n",
    "\n",
    "$$ y = w_0 + w_1 x_1 + w_2 x_2 + \\ldots + w_k x_k $$\n",
    "\n",
    "L<sub>1</sub> regularization defines model complexity as the sum of the absolute value of the feature weights (multiplied by a constant, $\\lambda$, which we will discuss later).\n",
    "\n",
    "$$L_1\\text{ }regularization\\text{ }penalty = \\lambda\\sum_{i=1}^k |w_i|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this formula, weights near zero have small effects on the model complexity, where larger weights have a larger effect. \n",
    "\n",
    "For example, if your model has the weights `[-0.5, -0.2, 0.5, 0.7, 1.0, 2.5]`, the L<sub>1</sub> regularization term is just the sum of absolute value of each of the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "weights = [-0.5, -0.2, 0.5, 0.7, 1.0, 2.5]\n",
    "print(sum(np.absolute(weights)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L<sub>2</sub> regularization represents model complexity as the sum of the squares of the feature weights (again multiplied by a constant, $\\lambda$).\n",
    "\n",
    "$$L_2\\text{ }regularization\\text{ }penalty = \\lambda\\sum_{i=1}^k w_i^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same weights from above, the L<sub>2</sub> regularization term can be computed with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(np.square(weights))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a. Adding regularization to a loss/cost function\n",
    "\n",
    "To use either of these regularization terms when fitting a model, the usual procedure is to add the regularization term to whatever loss function you want to use.  E.g., *total loss* = *loss* + *regularization penalty*.  Let's look at an example using linear regression.  Recall that the usual loss function for linear regression is the *mean square error*:\n",
    "\n",
    "$$ MSE = \\frac{1}{n} \\sum_{i=1}^n (y_i - (w_0 + w_1 x_{i,1} + w_2 x_{i,2} + \\ldots + w_k x_{i,k}))^2 $$\n",
    "\n",
    "Then, we add the regularization penalty to the loss function.  In the case of L<sub>1</sub> regularization, for instance, we would have:\n",
    "\n",
    "$$ MSE + \\lambda\\sum_{i=1}^k |w_i|$$\n",
    "\n",
    "\n",
    "\n",
    "### 2.b. Lambda\n",
    "\n",
    "The regularization parameter in the above formulas, $\\lambda$, allows you to adjust the balance between minimizing the loss function and penalizing overly complex models.  Increasing $\\lambda$ strengthens the regularization effect and will cause more of the model weights to be near zero, while decreasing $\\lambda$ places more emphasis on reducing the loss function.  Thus, the value of $\\lambda$ is very important during model fitting.  Too large a value for $\\lambda$ and your model might be overly simple and prone to underfitting.  Too small of a value and your model will be more complex, but you run the risk of overfitting. The optimal value of lambda is data-dependent and will usually need to be estimated in some way.\n",
    "\n",
    "\n",
    "### 2.c. Practical differences between L<sub>1</sub> and L<sub>2</sub> regularization\n",
    "\n",
    "L<sub>1</sub> and L<sub>2</sub> both can help prevent overfitting.  From a practical standpoint, perhaps the most important difference between the two is that L<sub>1</sub> regularization can help with *feature selection*.  As a consequence of the mathematical properties of L<sub>1</sub> regularization, L<sub>1</sub> regularization can result in models where some of the feature weights are 0, effectively removing those features from the model.  In contrast, L<sub>2</sub> regularization can decrease model weights but not drive them to 0.  L<sub>1</sub> regularization can also be more robust and resistant to larger outliers in the data.  On the other hand, L<sub>2</sub> regularization results in a minimization problem with a unique solution, which is not always the case for L<sub>1</sub> regularization.  Which regularization method is best depends on the specifics of the data, the modeling problem, and the goals of the analysis.\n",
    " \n",
    "\n",
    "## 3. Practice example / demonstration\n",
    "\n",
    "Now, let's use scikit-learn to take a look at these ideas in actual practice.  We'll start by working with a dataset called `regularization.csv` that you can find in the `nb-datasets` folder.\n",
    "\n",
    "### Step 1: Import the required libraries and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature_21</th>\n",
       "      <th>feature_22</th>\n",
       "      <th>feature_23</th>\n",
       "      <th>feature_24</th>\n",
       "      <th>feature_25</th>\n",
       "      <th>feature_26</th>\n",
       "      <th>feature_27</th>\n",
       "      <th>feature_28</th>\n",
       "      <th>feature_29</th>\n",
       "      <th>feature_30</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>72.456984</td>\n",
       "      <td>32.321012</td>\n",
       "      <td>65.689328</td>\n",
       "      <td>52.122727</td>\n",
       "      <td>63.399651</td>\n",
       "      <td>12.258814</td>\n",
       "      <td>17.043635</td>\n",
       "      <td>51.730222</td>\n",
       "      <td>62.860680</td>\n",
       "      <td>2.872635</td>\n",
       "      <td>...</td>\n",
       "      <td>76.858001</td>\n",
       "      <td>75.087967</td>\n",
       "      <td>38.094935</td>\n",
       "      <td>17.919735</td>\n",
       "      <td>37.380254</td>\n",
       "      <td>66.538720</td>\n",
       "      <td>23.598782</td>\n",
       "      <td>35.905618</td>\n",
       "      <td>43.137890</td>\n",
       "      <td>47.112709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>28.327361</td>\n",
       "      <td>30.668730</td>\n",
       "      <td>54.556984</td>\n",
       "      <td>58.913683</td>\n",
       "      <td>24.094272</td>\n",
       "      <td>69.364714</td>\n",
       "      <td>17.979578</td>\n",
       "      <td>56.464967</td>\n",
       "      <td>84.401995</td>\n",
       "      <td>51.842014</td>\n",
       "      <td>...</td>\n",
       "      <td>33.629986</td>\n",
       "      <td>75.302974</td>\n",
       "      <td>36.735844</td>\n",
       "      <td>56.038455</td>\n",
       "      <td>26.910583</td>\n",
       "      <td>94.038582</td>\n",
       "      <td>63.119873</td>\n",
       "      <td>48.105001</td>\n",
       "      <td>47.805156</td>\n",
       "      <td>73.564217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>57.966809</td>\n",
       "      <td>26.057286</td>\n",
       "      <td>26.943960</td>\n",
       "      <td>28.413263</td>\n",
       "      <td>47.659902</td>\n",
       "      <td>58.102919</td>\n",
       "      <td>60.523707</td>\n",
       "      <td>33.314868</td>\n",
       "      <td>33.338900</td>\n",
       "      <td>37.278545</td>\n",
       "      <td>...</td>\n",
       "      <td>45.392581</td>\n",
       "      <td>45.232257</td>\n",
       "      <td>93.975501</td>\n",
       "      <td>27.220155</td>\n",
       "      <td>39.191009</td>\n",
       "      <td>17.016580</td>\n",
       "      <td>47.965641</td>\n",
       "      <td>33.711634</td>\n",
       "      <td>69.566848</td>\n",
       "      <td>83.822185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>45.432275</td>\n",
       "      <td>33.974760</td>\n",
       "      <td>49.752550</td>\n",
       "      <td>37.456331</td>\n",
       "      <td>38.234778</td>\n",
       "      <td>41.094095</td>\n",
       "      <td>15.415567</td>\n",
       "      <td>61.188945</td>\n",
       "      <td>70.176046</td>\n",
       "      <td>26.425137</td>\n",
       "      <td>...</td>\n",
       "      <td>29.434284</td>\n",
       "      <td>47.596757</td>\n",
       "      <td>93.733349</td>\n",
       "      <td>33.764953</td>\n",
       "      <td>46.434659</td>\n",
       "      <td>51.489927</td>\n",
       "      <td>60.676096</td>\n",
       "      <td>48.719651</td>\n",
       "      <td>77.244083</td>\n",
       "      <td>46.888295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>41.635332</td>\n",
       "      <td>12.159012</td>\n",
       "      <td>60.621783</td>\n",
       "      <td>58.770553</td>\n",
       "      <td>35.291748</td>\n",
       "      <td>37.930786</td>\n",
       "      <td>35.609174</td>\n",
       "      <td>24.067184</td>\n",
       "      <td>29.361941</td>\n",
       "      <td>66.153336</td>\n",
       "      <td>...</td>\n",
       "      <td>69.537131</td>\n",
       "      <td>48.294237</td>\n",
       "      <td>85.607670</td>\n",
       "      <td>25.917772</td>\n",
       "      <td>35.023185</td>\n",
       "      <td>86.978918</td>\n",
       "      <td>24.345884</td>\n",
       "      <td>67.140613</td>\n",
       "      <td>21.886626</td>\n",
       "      <td>21.017448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    response  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
       "0  72.456984  32.321012  65.689328  52.122727  63.399651  12.258814   \n",
       "1  28.327361  30.668730  54.556984  58.913683  24.094272  69.364714   \n",
       "2  57.966809  26.057286  26.943960  28.413263  47.659902  58.102919   \n",
       "3  45.432275  33.974760  49.752550  37.456331  38.234778  41.094095   \n",
       "4  41.635332  12.159012  60.621783  58.770553  35.291748  37.930786   \n",
       "\n",
       "   feature_6  feature_7  feature_8  feature_9  ...  feature_21  feature_22  \\\n",
       "0  17.043635  51.730222  62.860680   2.872635  ...   76.858001   75.087967   \n",
       "1  17.979578  56.464967  84.401995  51.842014  ...   33.629986   75.302974   \n",
       "2  60.523707  33.314868  33.338900  37.278545  ...   45.392581   45.232257   \n",
       "3  15.415567  61.188945  70.176046  26.425137  ...   29.434284   47.596757   \n",
       "4  35.609174  24.067184  29.361941  66.153336  ...   69.537131   48.294237   \n",
       "\n",
       "   feature_23  feature_24  feature_25  feature_26  feature_27  feature_28  \\\n",
       "0   38.094935   17.919735   37.380254   66.538720   23.598782   35.905618   \n",
       "1   36.735844   56.038455   26.910583   94.038582   63.119873   48.105001   \n",
       "2   93.975501   27.220155   39.191009   17.016580   47.965641   33.711634   \n",
       "3   93.733349   33.764953   46.434659   51.489927   60.676096   48.719651   \n",
       "4   85.607670   25.917772   35.023185   86.978918   24.345884   67.140613   \n",
       "\n",
       "   feature_29  feature_30  \n",
       "0   43.137890   47.112709  \n",
       "1   47.805156   73.564217  \n",
       "2   69.566848   83.822185  \n",
       "3   77.244083   46.888295  \n",
       "4   21.886626   21.017448  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "rdata = pd.read_csv('../nb-datasets/regularization.csv')\n",
    "rdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Prepare the training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Separate the x and y values.\n",
    "x = rdata.drop(columns='response')\n",
    "y = rdata['response']\n",
    "\n",
    "# Split the train and test sets.  Use 25% of the data for testing.\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.5, random_state=7\n",
    ")\n",
    "len(rdata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Fit a standard linear regression model\n",
    "\n",
    "We'll give regular old non-regularized linear regression a try first.  As a reminder, we'll fit the model using the training data and then check how it performs on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2: 0.9999392834150598\n",
      "Train loss: 0.029419148916598636\n",
      "\n",
      "Coefficients: [-4.12797020e-02 -1.79616277e-01 -4.99773043e-02  1.17300976e+00\n",
      "  2.82296227e-02  2.27359809e-01 -7.21474264e-03  5.07660342e-02\n",
      " -1.35590182e-01  1.09717013e-01 -1.72193496e-01 -2.14314632e-01\n",
      "  1.11973445e-01 -4.24759522e-02 -1.12168799e-01 -8.45964728e-02\n",
      "  5.65948007e-02 -1.88272052e-01  3.31274473e-01 -2.79209727e-02\n",
      "  1.16344162e-02  4.69337409e-02  2.38795254e-02 -1.90923687e-02\n",
      "  6.35118218e-04 -6.35080596e-02  3.56423726e-02  5.60962269e-02\n",
      "  4.40148056e-02  3.75539252e-02]\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Take a look at the model's R^2 score and training loss (mean squared error).\n",
    "print('Train R2:', model.score(x_train, y_train))\n",
    "train_loss = mse(y_train, model.predict(x_train))\n",
    "print('Train loss:', train_loss)\n",
    "\n",
    "# Let's also have a look at the model's coefficients.\n",
    "print('\\nCoefficients:', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Check the model's performance on the testing data\n",
    "\n",
    "Our model predicts the training responses extremely well (almost perfectly!).  Let's see how it does on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test R2: 0.49778230757655045\n",
      "Test loss: 199.7198626958782\n"
     ]
    }
   ],
   "source": [
    "print('Test R2:', model.score(x_test, y_test))\n",
    "\n",
    "test_loss = mse(y_test, model.predict(x_test))\n",
    "print('Test loss:', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh.  Predictions on \"new\" data are terrible!  This result indicates that our model is probably badly overfit to the training data.\n",
    "\n",
    "\n",
    "### Step 5: Try using L<sub>1</sub> regularization\n",
    "\n",
    "Let's see if we can use L<sub>1</sub> regularization to fix this.  In scikit-learn, the `Lasso` object provides linear regression with L<sub>1</sub> regularization.  We use it in much the same way as `LinearRegression`, but we need to provide a value for the parameter `alpha`, which corresponds with the parameter $\\lambda$ discussed in the regularization formulas above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2: 0.9897749936281043\n",
      "Train loss: 4.954346253562866\n",
      "\n",
      "Test R2: 0.9865604019011951\n",
      "Test loss: 5.344603998375144\n",
      "\n",
      "Coefficients: [ 0.         -0.          0.          1.11487814  0.          0.\n",
      " -0.         -0.         -0.         -0.          0.         -0.\n",
      "  0.         -0.          0.         -0.          0.          0.\n",
      "  0.          0.         -0.          0.         -0.         -0.\n",
      " -0.         -0.         -0.         -0.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "model = Lasso(alpha=22.0)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print('Train R2:', model.score(x_train, y_train))\n",
    "train_loss = mse(y_train, model.predict(x_train))\n",
    "print('Train loss:', train_loss)\n",
    "\n",
    "print('\\nTest R2:', model.score(x_test, y_test))\n",
    "test_loss = mse(y_test, model.predict(x_test))\n",
    "print('Test loss:', test_loss)\n",
    "\n",
    "print('\\nCoefficients:', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great success!  As is typical with regularization, the model's performance on the training data has decreased a little bit, but its performance on the testing data has massively improved.  The model's performance on \"new\" data is now competitive with its performance on the training data, which is about the best we can hope for.\n",
    "\n",
    "Note also that with a regularization parameter (`alpha`/$\\lambda$ ) of 22.0, we've applied fairly aggressive regularization to our model fit, which has resulted in the L<sub>1</sub> regularization driving *nearly all* of the model's coefficients/feature weights to 0!  Only the coefficient for `feature_4` remains.  In other words, the L<sub>1</sub> regularization has drastically simplified our model by eliminating all features except for `feature_4`.\n",
    "\n",
    "Does that result make sense?  As it turns out, it does.  The dataset we've been using is an artificial dataset that I created for this exercise, so we know the true relationship of the features to the response.  As suggested by the regularized coefficient estimates, `feature_4` is the only feature that has any relationship with the response variable.  In reality, `response` is equal to `feature_4` * 1.2 plus a small amount of random noise.  *All* of the other features are entirely random and have no relationship to `response` at all.\n",
    "\n",
    "Although this is a contrived dataset, it is not as unrealistic as it might at first seem.  With complex real-world data, we have often have a large number of candidate features/predictor variables, and we don't know which, if any, of those features have any real relationship with the response variable.  In those situations, overfitting is a real concern, especially if the number of observations is relatively small, and regularization can help prevent us from drawing the wrong conclusions from our data.\n",
    "\n",
    "What about the value of the regularization parameter, `alpha`/$\\lambda$?  Here, I deliberately chose a value (20.0) that I knew would work well given the truth behind the dataset.  With real data, we never have that luxury, and more sophisticated methods (that we don't have time to present in this short course) are required to select an appropriate value of $\\lambda$.\n",
    "\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Try experimenting with the value of the regularization parameter in the code above.  How does changing the value of alpha affect the results?  When do you get results that are misleading or just plain wrong?\n",
    "\n",
    "\n",
    "### Step 6: Try using L<sub>2</sub> regularization\n",
    "\n",
    "We can also analyze these data using L<sub>2</sub> regularization.  To do that, we'll use a scikit-learn object called `Ridge` that implements linear regression with L<sub>2</sub> regularization.  The interface of `Ridge` is exactly the same as the interface of `Lasso`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(alpha=1.0)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print('Train R2:', model.score(x_train, y_train))\n",
    "train_loss = mse(y_train, model.predict(x_train))\n",
    "print('Train loss:', train_loss)\n",
    "\n",
    "print('\\nTest R2:', model.score(x_test, y_test))\n",
    "test_loss = mse(y_test, model.predict(x_test))\n",
    "print('Test loss:', test_loss)\n",
    "\n",
    "print('\\nCoefficients:', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This illustrates a key difference between L<sub>1</sub> regularization and L<sub>2</sub> regularization: L<sub>1</sub> regularization is able to drive feature weights/model coefficients to 0 and effectively remove features from a model, but L<sub>2</sub> regularization cannot, even when doing so would be appropriate.\n",
    "\n",
    "Consequently, for these data, L<sub>2</sub> regularization helps a bit with overfitting, but it clearly does not perform nearly as well as L<sub>1</sub> regularization.  That won't always be the case, though.  This dataset is an intentionally extreme examle that suits L<sub>1</sub> regularization especially well.  For datasets where all or most of the features really do have some connection to the response or where collinearity is a concern, L<sub>2</sub> regularization might be a better choice.\n",
    "\n",
    "\n",
    "## 4.  Practice example using real data\n",
    "\n",
    "As a final example, let's try using regularization on a real dataset.  We'll again use the iris dataset that you've already seen in previous lessons.  We might not have time for this example during the workshop, and if not, I encourage you to explore it on your own.\n",
    "\n",
    "### Steps 1 and 2: Load the data and split out training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata = pd.read_csv('../nb-datasets/iris_dataset.csv')\n",
    "idata['species'] = idata['species'].astype('category')\n",
    "\n",
    "# Convert the categorical variable \"species\" to 1-hot encoding (AKA \"dummy variables\"),\n",
    "# but eliminate the first dummy variable because it is collinear with the other two\n",
    "# and does not provide any additional information.\n",
    "idata_enc = pd.get_dummies(idata, drop_first=True)\n",
    "\n",
    "# Separate the x and y values.\n",
    "x = idata_enc.drop(columns='petal_length')\n",
    "y = idata_enc['petal_length']\n",
    "\n",
    "# Split the train and test sets.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n",
    "\n",
    "# See what we have.\n",
    "idata_enc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps 3 and 4: Give standard linear regression a try\n",
    "\n",
    "We'll fit a standard linear regression model and check its performance on the testing data.  This will give us a baseline to compare to the regularization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print('Train R2:', model.score(x_train, y_train))\n",
    "train_loss = mse(y_train, model.predict(x_train))\n",
    "print('Train loss:', train_loss)\n",
    "\n",
    "print('\\nTest R2:', model.score(x_test, y_test))\n",
    "test_loss = mse(y_test, model.predict(x_test))\n",
    "print('Test loss:', test_loss)\n",
    "\n",
    "print('Coefficients:', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: L<sub>1</sub> regularization\n",
    "\n",
    "The results above *strongly* suggest that overfitting really isn't much of a problem here.  We have a relatively small number of features (i.e., parameters we must estimate) compared to the number of observations, so that result makes sense.  Nevertheless, let's try fitting the model using L<sub>1</sub> regularization and see what happens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(alpha=0.01)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print('Train R2:', model.score(x_train, y_train))\n",
    "train_loss = mse(y_train, model.predict(x_train))\n",
    "print('Train loss:', train_loss)\n",
    "\n",
    "print('\\nTest R2:', model.score(x_test, y_test))\n",
    "test_loss = mse(y_test, model.predict(x_test))\n",
    "print('Test loss:', test_loss)\n",
    "\n",
    "print('\\nCoefficients:', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: L<sub>2</sub> regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(alpha=2.0)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print('Train R2:', model.score(x_train, y_train))\n",
    "train_loss = mse(y_train, model.predict(x_train))\n",
    "print('Train loss:', train_loss)\n",
    "\n",
    "print('\\nTest R2:', model.score(x_test, y_test))\n",
    "test_loss = mse(y_test, model.predict(x_test))\n",
    "print('Test loss:', test_loss)\n",
    "\n",
    "print('\\nCoefficients:', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Try experimenting with the value of `alpha`/$\\lambda$ in the code above for both L<sub>1</sub> regularization and L<sub>2</sub> regularization.  As you do so, consider these questions:\n",
    "\n",
    "1. How does changing the value of the regularization parameter affect the coefficient weights and training/test performance?\n",
    "2. What values of the regularization parameter give you the best test accuracy?\n",
    "3. For these data, does L<sub>1</sub> or L<sub>2</sub> regularization perform better?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
