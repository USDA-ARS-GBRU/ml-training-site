{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "Author: Carson Andorf\n",
    "\n",
    "Up to this point, we improved our machine learning models by minimizing training loss.  This can work very well, but there are many instances where making the training loss as small as possible can actually cause the model's performance on validation data (i.e., the validation loss) *to get worse* (see the figure below)!   This problem is referred to as overtraining or overfitting.  Overfitting occurs when the model starts to fit to the uniqueness or quirkiness of the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Two loss function](../nb-images/Regularization.svg)\n",
    "<div style=\"text-align: right\"> (Image from Google machine learning crash course) </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, consider this simple dataset that contains information about insect, fish, and bird species and whether or not they can fly:\n",
    "\n",
    "|Name|Class|Can fly|\n",
    "|:--:|:---:|:-----:|\n",
    "|Pileated woodpecker|Birds|Yes|\n",
    "|Emu|Birds|No|\n",
    "|Northern cardinal|Birds|Yes|\n",
    "|Blacktip shark|Cartilaginous fishes|No|\n",
    "|Bluntnose stingray|Cartilaginous fishes|No|\n",
    "|Black drum|Bony fishes|No|\n",
    "|Florida carpenter ant|Insects|No|\n",
    "|Periodical cicada|Insects|Yes|\n",
    "|Luna moth|Insects|Yes|\n",
    "\n",
    "Your task is to develop a model to classify whether or not an animal can fly, based on information available in the dataset.  Here is a relatively simple model based on these data:\n",
    "  * If the animal is a bird or an insect, predict that it can fly.\n",
    "  * Otherwise, predict that it cannot fly.\n",
    "\n",
    "This model is imperfect.  Indeed, it misclassifies 2 of the examples in the training data.  We can try developing a more complex model to reduce our training loss:\n",
    "  * If the species is a bird and has a one-word name, predict that it cannot fly.\n",
    "  * If it is a bird with a two-word name, predict that it can fly.\n",
    "  * If it is an insect with a three-word name, predict that it cannot fly.\n",
    "  * If it is an insect with a two-word name, predict that it can fly.\n",
    "  * Otherwise, predict that it cannot fly.\n",
    "\n",
    "Aha!  That model classifies each training example perfectly!  When presented with new examples, however (e.g., \"albatross\" or \"zebra swallowtail butterfly\"), the more complex model will often fail spectacularly while the simpler model, although imperfect, will perform relatively well.  Clearly, our more complex model is disastrously overfitted to the training data.\n",
    "\n",
    "The bottom line is that we want our models to be general enough to work well on new examples.  Methods to help prevent overfitting are collectively referred to as *regularization* techniques.  Regularization essentially attempts to apply Occamâ€™s razor on the model: a less complex machine learning model with good empirical results is preferred over a more complex model.  Another way to think about regularization is that you should not trust your training examples too much.\n",
    "\n",
    "There are a variety of ways to implement regularization to prevent overfitting.  One option is to stop training early before overfitting happens. In the figure above, you would try to stop at the point on the red curve where validation loss begins to increase.  Another approach, which is often easier to implement in practice, is to explicitly penalize model complexity in the model-fitting procedure.  Instead of just minimizing loss, we minimize `loss + complexity`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L<sub>1</sub> and L<sub>2</sub> regularization\n",
    "\n",
    "For this lesson, we will focus on two widely used regularization methods: L<sub>1</sub> and L<sub>2</sub> regularization.  Both of these methods represent model complexity as a function of the model's feature weights.\n",
    "\n",
    "L<sub>1</sub> regularization defines model complexity as the sum of the absolute value of the feature weights (multiplied by a constant, *lambda*, which we will discuss later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![L1 regularization](../nb-images/reg_formula1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this formula, weights near zero have small effects on the model complexity, where larger weights have a larger effect. \n",
    "\n",
    "For example, if your model has the following weights [-0.5,-0.2,0.5,0.7,1.0,2.5], the L<sub>1</sub> regularization term is just the sum of absolute value of each of the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "weights = [-0.5, -0.2, 0.5, 0.7, 1.0, 2.5]\n",
    "print(sum(np.absolute(weights)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L<sub>2</sub> regularization represents model complexity as the sum of the squares of the feature weights (again multiplied by a constant, *lambda*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![L2 regularization](../nb-images/reg_formula2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same weights from above, the L<sub>2</sub> regularization term can be computed with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.280000000000001\n"
     ]
    }
   ],
   "source": [
    "print(sum(np.square(weights))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use either of these regularization terms when fitting a model, the usual procedure is to add the regularization term to whatever loss function you want to use.  E.g., *total loss* = *loss* + *regularization term*.\n",
    "\n",
    "\n",
    "## Lambda\n",
    "\n",
    "The regularization parameter in the above formulas, *lambda*, allows you to adjust the balance between minimizing the loss function and penalizing overly complex models.  Increasing *lambda* strengthens the regularization effect and will cause more of the model weights to be near zero, while decreasing *lambda* places more emphasis on reducing the loss function.  Thus, the value of *lambda* is very important during model fitting.  Too large a value for *lambda* and your model might be overly simple and prone to underfitting.  Too small of a value and your model will be more complex, but you run the risk of overfitting. The optimal value of lambda is data-dependent and will usually need to be estimated in some way.\n",
    "\n",
    "\n",
    "## Practical differences between L<sub>1</sub> and L<sub>2</sub> regularization\n",
    "\n",
    "L<sub>1</sub> and L<sub>2</sub> both can help prevent overfitting.  From a practical standpoint, perhaps the most important difference between the two is that L<sub>1</sub> regularization can help with *feature selection*.  As a consequence of the mathematical properties of L<sub>1</sub> regularization, L<sub>1</sub> regularization can result in models where some of the feature weights are 0, effectively removing those features from the model.  In contrast, L<sub>2</sub> regularization can decrease model weights but not drive them to 0.  L<sub>1</sub> regularization can also be more robust and resistant to larger outliers in the data.  On the other hand, L<sub>2</sub> regularization results in a minimization problem with a unique solution, which is not always the case for L<sub>1</sub> regularization.  Which regularization method is best depends on the specifics of the data, the modeling problem, and the goals of the analysis.\n",
    " \n",
    "\n",
    "## Practice Example\n",
    "\n",
    "Regularization is supported in scikit-learn.  For example, the `LogisticRegression` model supports both L<sub>1</sub>- and L<sub>2</sub>-regularized regression.  The two major parameters are `penalty` and `C`.  The paraeter `penalty` is used to specify what type of regression to use (e.g. `'l2'`).  The parameter `C` is used as the inverse of the regularization parameter *lambda*.  Therefore, smaller values of `C` represent stronger regularization.   \n",
    "\n",
    "The following programming exercise will show the effect of changing the type and strength of regularization on the logistic regression models using the Iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error as mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:  Load the data set and prepare the features and response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72     4.9\n",
      "80     3.8\n",
      "101    5.1\n",
      "7      1.5\n",
      "6      1.4\n",
      "      ... \n",
      "29     1.6\n",
      "147    5.2\n",
      "40     1.3\n",
      "32     1.5\n",
      "129    5.8\n",
      "Name: petal_length, Length: 112, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>species_versicolor</th>\n",
       "      <th>species_virginica</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>5.5</td>\n",
       "      <td>2.4</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_width  species_versicolor  \\\n",
       "72            6.3          2.5          1.5                   1   \n",
       "80            5.5          2.4          1.1                   1   \n",
       "101           5.8          2.7          1.9                   0   \n",
       "7             5.0          3.4          0.2                   0   \n",
       "6             4.6          3.4          0.3                   0   \n",
       "\n",
       "     species_virginica  \n",
       "72                   0  \n",
       "80                   0  \n",
       "101                  1  \n",
       "7                    0  \n",
       "6                    0  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idata = pd.read_csv('../nb-datasets/iris_dataset.csv')\n",
    "idata['species'] = idata['species'].astype('category')\n",
    "\n",
    "# Convert the categorical variable \"species\" to 1-hot encoding (AKA \"dummy variables\"),\n",
    "# but eliminate the first dummy variable because it is collinear with the other two\n",
    "# and does not provide any additional information.\n",
    "idata_enc = pd.get_dummies(idata, drop_first=True)\n",
    "\n",
    "# Separate the x and y values.\n",
    "x = idata_enc.drop(columns='petal_length')\n",
    "y = idata_enc['petal_length']\n",
    "\n",
    "# Split the train and test sets.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n",
    "\n",
    "# See what we have.\n",
    "print(y_train)\n",
    "x_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Give standard linear regression a try\n",
    "\n",
    "We'll give regular old non-regularized linear regression a try first.  This will give us a baseline to compare to the regularization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2: 0.9793028093800101\n",
      "Test R2: 0.971832233015809\n",
      "Train loss: 0.06542523192185211\n",
      "Test loss: 0.07901487787870036\n",
      "Coefficients: [ 0.65503965 -0.2961708   0.47704172  1.48747658  2.03756475]\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print('Train R2:', model.score(x_train, y_train))\n",
    "print('Test R2:', model.score(x_test, y_test))\n",
    "\n",
    "train_loss = mse(y_train, model.predict(x_train))\n",
    "test_loss = mse(y_test, model.predict(x_test))\n",
    "print('Train loss:', train_loss)\n",
    "print('Test loss:', test_loss)\n",
    "\n",
    "print('Coefficients:', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Let's give L<sub>2</sub> regularization a try\n",
    "\n",
    "The results above suggest that overfitting really isn't too much of a problem here.  We have a relatively small number of features (i.e., parameters we must estimate) compared to the number of observations, so this result should not be very suprising.  Nevertheless, let's try using L<sub>2</sub> regularization and see what happens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2: 0.9755523410003801\n",
      "Test R2: 0.9685744986895839\n",
      "Train loss: 0.07959825668785966\n",
      "Test loss: 0.08148606374415827\n",
      "Coefficients: [ 0.74299035 -0.46691584  1.06369227  0.55090305  0.73545887]\n"
     ]
    }
   ],
   "source": [
    "model = Ridge(alpha = 2.0)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print('Train R2:', model.score(x_train, y_train))\n",
    "print('Test R2:', model.score(x_test, y_test))\n",
    "\n",
    "train_loss = mse(y_train, model.predict(x_train))\n",
    "test_loss = mse(y_test, model.predict(x_test))\n",
    "print('Train loss:', train_loss)\n",
    "print('Test loss:', test_loss)\n",
    "\n",
    "print('Coefficients:', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Finally, let's give L<sub>1</sub> regularization a try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train R2: 0.9510922774078785\n",
      "Test R2: 0.9511275681292637\n",
      "Train loss: 0.1592369010450801\n",
      "Test loss: 0.12672580969872624\n",
      "Coefficients: [ 0.59186282 -0.13893823  1.51562559  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "model = Lasso(alpha = 0.1)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print('Train R2:', model.score(x_train, y_train))\n",
    "print('Test R2:', model.score(x_test, y_test))\n",
    "\n",
    "train_loss = mse(y_train, model.predict(x_train))\n",
    "test_loss = mse(y_test, model.predict(x_test))\n",
    "print('Train loss:', train_loss)\n",
    "print('Test loss:', test_loss)\n",
    "\n",
    "print('Coefficients:', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After completing the exercise, answer the questions below\n",
    "\n",
    "1. How did changing the value of C affect the coefficient weights and training/test performance?\n",
    "2. What value of C gave you the best test accuracy?\n",
    "3. For this problem, do you recommend a high or low regularization strength?  Does this make sense based on the data?\n",
    "4. Switch the reg_model from 'l1' to '12'. Which regularization (l1 or l2) had models with more zero weights?\n",
    "5. How could you use a sparse matrix (i.e. a matrix with a lot of zero weights) for feature selection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
