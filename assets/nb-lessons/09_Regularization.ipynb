{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "Author: Carson Andorf\n",
    "\n",
    "Up to this point, we improved our machine learning models by minimizing training loss.  This works very well, but there are many instances where additional iterations to our model will continue to decrease training loss, but the actual performance on validation data (validation loss) will actually increase (See figure below).   This is referred to as overtraining or overfitting. Overfitting occurs when the model starts to fit to the uniqueness or quirkiness of the training set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Two loss function](../nb-images/Regularization.svg)\n",
    "<div style=\"text-align: right\"> (Image from Google machine learning crash course) </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine if you were trying to build a model that learns English, but your training set was based on words spoken from teenagers. In the early iterations, the model would quickly reduce training loss, but the model could eventually reach a point of overfitting based on slang, abbreviations, and potential limited vocabulary unique to how teenagers speak. We want our model to be general enough to still work well on new examples. A method to reduce overfitting is called Regularization. Regularization essentially attempts to apply Occam’s razor on the model: a less complex machine learning model with good empirical results is preferred over a more complex model. Another way to think about regularization is that you should not trust your training examples too much.\n",
    "\n",
    "There are two main methods to use regularization to prevent overfitting. The first in to stop training early before overfitting happens. In the figure above, you would try to stop at the point on the red curve where validation loss begins to increase. This method is difficult to implement in practice. The second method is to penalize the model complexity. Instead of just minimizing loss (empirical risk minimization), we will minimize Loss+Complexity. This is called structural risk minimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 and L2 Regularization\n",
    "\n",
    "There are two common ways to represent model complexity:\n",
    "\n",
    "  * As a function of all the feature weights\n",
    "  * As a function of the total number of nonzero feature weights\n",
    "\n",
    "We will focus on two regularization methods that quantify complexity as a function of weights: L1 and L2 regularization.  \n",
    "\n",
    "L1  defines the complexity as the sum of the absolute value of the feature weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![L1 regularization](../nb-images/reg_formula1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this formula, weights near zero have small effects on the model complexity, where larger weights have a larger effect. \n",
    "\n",
    "For example, if your model has the following weights [-0.5,-0.2,0.5,0.7,1.0,2.5], the L1 regularization term is just the sum of absolute value of each of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-0.5| + |-0.2| + |0.5| + |0.7| + |1.0| + |2.5| = 5.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "weights = [-0.5,-0.2,0.5,0.7,1.0,2.5]\n",
    "print (\"|-0.5| + |-0.2| + |0.5| + |0.7| + |1.0| + |2.5| =\", sum(np.absolute(weights))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 defines the regularization term as the sum of the squares of the feature weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![L2 regularization](../nb-images/reg_formula2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same weights from above, the L2 regularization can be computed with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5^2 + -0.2^2 + 0.5^2 + 0.7^2 + 1.0^2 + 2.5^2 = 8.280000000000001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "weights = [-0.5,-0.2,0.5,0.7,1.0,2.5]\n",
    "weights_squared = np.square(weights)\n",
    "print (\"-0.5^2 + -0.2^2 + 0.5^2 + 0.7^2 + 1.0^2 + 2.5^2 =\", sum(np.square(weights))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda\n",
    "Lambda (also called regularization rate) in the above formulas allows you to choose how much regularization you want to apply to your model.  Increasing Lambda strengthens the regularization effect and will cause more of the weights to be near zero.  You do want to be careful when choosing Lambda.  Too large of a value for Lambda and your model will be simple, but this could oversimplify your model and cause underfitting.  Too small of a value and your model will be more complex, but you run the risk of overfitting. The optimal value of lambda is data-dependent, so you may need to do some tuning to find the ideal value.\n",
    "\n",
    "## L1 vs L2\n",
    "Both L1 and L2 can help with overfitting. L1 can also help with feature selection as they tend to produce more sparse weight matrices allowing you to choose non-zero weight features.  L1 is also more robust and resistant to larger outliers in the data.  L2 will only have one solution (L2 may have multiple solutions) and is more computationally efficient to compute. L2 is better in practice if the model is a function of all of the input features.  \n",
    " \n",
    "\n",
    "## Practice Example\n",
    "Regularization is supported in scikit-learn. For example, the LogisticRegression model supports both L1 and L2 regression. The two major parameters are penalty and C.  Penalty is used to specify what type of regression to use (e.g. ‘l2’)  The parameter C is used as the inverse of the regularization strength (e.g. 0.1).  Smaller values of C represent stronger regularization.   \n",
    "\n",
    "The following programming exercise will show the effect of changing the type and strength of regularization on the logistic regression models using the Iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import pandas\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:  Load the data set and create your features and target classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import iris data from scikit and data preparation\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Create X from the features\n",
    "iris_X = iris.data\n",
    "\n",
    "# Create y from traget class\n",
    "iris_y = iris.target\n",
    "\n",
    "# Add the next two lines to make this a binary classification problem (remove 1 of the 3 classes)\n",
    "#iris_X = iris_X[iris_y != 2]\n",
    "#iris_y = iris_y[iris_y != 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]]\n"
     ]
    }
   ],
   "source": [
    "# check data shape\n",
    "print(iris_X.data.shape)\n",
    "\n",
    "# View first 5 rows\n",
    "print(iris_X[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "# View classes\n",
    "print(iris_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3:  Prepare your training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting into train and test data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris_X, iris_y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize Features\n",
    "\n",
    "The penalty is the sum of the absolute value of the weights, so we need to scale the data so the weights are all based on the same scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scaler object\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Fit the scaler to the training data and transform\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "\n",
    "# Apply the scaler to the test data\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4:  Apply Regularization.  \n",
    "\n",
    "In this example, we will loop through several examples of regularization strength which is stored in the array C.  L1 and L2 regularization can be changed by the variable reg_model. Pay careful attention to how C and the choice of regularization method affect your model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 5\n",
      "////////////////////////////////////////////////////\n",
      "Coefficient of each feature:\n",
      "[[ 0.          3.4247781  -3.84598588  0.        ]\n",
      " [-0.02944101 -2.37470414  1.06330456 -2.42186994]\n",
      " [-4.2998018  -2.97935922  6.47634691  5.72496752]]\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13  0  0]\n",
      " [ 0 14  2]\n",
      " [ 0  0  9]]\n",
      "Test accuracy: 0.9473684210526315\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37  0  0]\n",
      " [ 0 33  1]\n",
      " [ 0  0 41]]\n",
      "Training accuracy: 0.9910714285714286\n",
      "////////////////////////////////////////////////////\n",
      "\n",
      "C: 1\n",
      "////////////////////////////////////////////////////\n",
      "Coefficient of each feature:\n",
      "[[ 0.          2.34857721 -2.64903276  0.        ]\n",
      " [ 0.41214186 -1.4761859   0.44239796 -1.18472386]\n",
      " [-3.07915763 -1.46576355  3.76849411  3.21530389]]\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13  0  0]\n",
      " [ 0 12  4]\n",
      " [ 0  0  9]]\n",
      "Test accuracy: 0.8947368421052632\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37  0  0]\n",
      " [ 0 31  3]\n",
      " [ 0  0 41]]\n",
      "Training accuracy: 0.9732142857142857\n",
      "////////////////////////////////////////////////////\n",
      "\n",
      "C: 0.1\n",
      "////////////////////////////////////////////////////\n",
      "Coefficient of each feature:\n",
      "[[ 0.          0.93808706 -1.15714799  0.        ]\n",
      " [ 0.         -0.26929983  0.00159065  0.        ]\n",
      " [-0.79977444  0.          1.07219186  0.        ]]\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13  0  0]\n",
      " [ 0  0 16]\n",
      " [ 0  0  9]]\n",
      "Test accuracy: 0.5789473684210527\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37  0  0]\n",
      " [ 0  2 32]\n",
      " [ 0  0 41]]\n",
      "Training accuracy: 0.7142857142857143\n",
      "////////////////////////////////////////////////////\n",
      "\n",
      "C: 0.001\n",
      "////////////////////////////////////////////////////\n",
      "Coefficient of each feature:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13  0  0]\n",
      " [16  0  0]\n",
      " [ 9  0  0]]\n",
      "Test accuracy: 0.34210526315789475\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37  0  0]\n",
      " [34  0  0]\n",
      " [41  0  0]]\n",
      "Training accuracy: 0.33035714285714285\n",
      "////////////////////////////////////////////////////\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "C = [5, 1, .1, .001]\n",
    "reg_model = \"l1\"\n",
    "#reg_model = \"l2\"\n",
    "\n",
    "for c in C:\n",
    "    model = LogisticRegression(penalty=reg_model, C=c, solver='liblinear', multi_class='auto')\n",
    "    model.fit(X_train, y_train)\n",
    "    predicted_train = model.predict(X_train)\n",
    "    predicted = model.predict(X_test)\n",
    "    print('C:', c)\n",
    "    print('////////////////////////////////////////////////////')\n",
    "    print('Coefficient of each feature:')\n",
    "    print(model.coef_)\n",
    "    print('')\n",
    "    print('Confusion Matrix:')\n",
    "    print (metrics.confusion_matrix(y_test, predicted))\n",
    "    print ('Test accuracy:', metrics.accuracy_score(y_test, predicted)) \n",
    "    print('')\n",
    "    print('Confusion Matrix:')\n",
    "    print (metrics.confusion_matrix(y_train, predicted_train))\n",
    "    print ('Training accuracy:', metrics.accuracy_score(y_train, predicted_train)) \n",
    "    print('////////////////////////////////////////////////////')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After completing the exercise, answer the questions below?\n",
    "\n",
    "1. How did changing the value of C affect the coefficient weights and training/test performance?\n",
    "2. What value of C gave you the best test accuracy?\n",
    "3. For this problem, do you recommend a high or low regularization strength?  Does this make sense based on the data?\n",
    "4. Switch the reg_model from 'l1' to '12'. Which regularization (l1 or l2) had models with more zero weights?\n",
    "5. How could you use a sparse matrix (i.e. a matrix with a lot of zero weights) for feature selection?\n",
    "\n",
    "Bonus: Can you make this a binary classification problem?  How does that change your results? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
