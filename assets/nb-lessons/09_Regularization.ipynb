{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "Authors: Brian Stucky, Carson Andorf\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "Up to this point, we improved our machine learning models by minimizing training loss.  This can work very well, but there are many instances where making the training loss as small as possible can actually cause the model's performance on validation data (i.e., the validation loss) *to get worse* (see the figure below)!   This problem is referred to as overtraining or overfitting.  Overfitting occurs when the model starts to fit to characteristics of the training set that don't generalize to new data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Two loss function](../nb-images/Regularization.svg)\n",
    "<div style=\"text-align: right\"> (Image from Google machine learning crash course) </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For instance, consider this simple dataset that contains information about insect, fish, and bird species and whether or not they can fly:\n",
    "\n",
    "|Name|Class|Can fly|\n",
    "|:--:|:---:|:-----:|\n",
    "|Pileated woodpecker|Birds|Yes|\n",
    "|Emu|Birds|No|\n",
    "|Northern cardinal|Birds|Yes|\n",
    "|Blacktip shark|Cartilaginous fishes|No|\n",
    "|Bluntnose stingray|Cartilaginous fishes|No|\n",
    "|Black drum|Bony fishes|No|\n",
    "|Florida carpenter ant|Insects|No|\n",
    "|Periodical cicada|Insects|Yes|\n",
    "|Luna moth|Insects|Yes|\n",
    "\n",
    "Your task is to develop a model to classify whether or not an animal can fly, based on information available in the dataset.  Here is a relatively simple model based on these data:\n",
    "  * If the animal is a bird or an insect, predict that it can fly.\n",
    "  * Otherwise, predict that it cannot fly.\n",
    "\n",
    "This model is imperfect.  Indeed, it misclassifies 2 of the examples in the training data.  We can try developing a more complex model to reduce our training loss:\n",
    "  * If the species is a bird and has a one-word name, predict that it cannot fly.\n",
    "  * If it is a bird with a two-word name, predict that it can fly.\n",
    "  * If it is an insect with a three-word name, predict that it cannot fly.\n",
    "  * If it is an insect with a two-word name, predict that it can fly.\n",
    "  * Otherwise, predict that it cannot fly.\n",
    "\n",
    "Aha!  That model classifies each training example perfectly!\n",
    "\n",
    "When presented with new examples, however (e.g., \"albatross\" or \"zebra swallowtail butterfly\"), the more complex model will often fail spectacularly while the simpler model, although imperfect, will perform relatively well.  Clearly, our more complex model is disastrously overfitted to the training data.\n",
    "\n",
    "The bottom line is that we want our models to be general enough to work well on new examples.  Methods to help prevent overfitting are collectively referred to as *regularization* techniques.  Regularization essentially attempts to apply Occamâ€™s razor on the model: a less complex machine learning model with good empirical results is preferred over a more complex model.  Another way to think about regularization is that you should not trust your training examples too much.\n",
    "\n",
    "There are a variety of ways to implement regularization to prevent overfitting.  One option is to stop training early before overfitting happens. In the figure above, you would try to stop at the point on the red curve where validation loss begins to increase.  Another approach, which is often easier to implement in practice, is to explicitly penalize model complexity in the model-fitting procedure.  Instead of just minimizing loss, we minimize `loss + complexity`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. L<sub>1</sub> and L<sub>2</sub> regularization\n",
    "\n",
    "For this lesson, we will focus on two widely used regularization methods: L<sub>1</sub> and L<sub>2</sub> regularization.  Both of these methods represent model complexity as a function of the model's feature weights.\n",
    "\n",
    "L<sub>1</sub> regularization defines model complexity as the sum of the absolute value of the feature weights (multiplied by a constant, *lambda*, which we will discuss later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![L1 regularization](../nb-images/reg_formula1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this formula, weights near zero have small effects on the model complexity, where larger weights have a larger effect. \n",
    "\n",
    "For example, if your model has the following weights [-0.5,-0.2,0.5,0.7,1.0,2.5], the L<sub>1</sub> regularization term is just the sum of absolute value of each of the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "weights = [-0.5, -0.2, 0.5, 0.7, 1.0, 2.5]\n",
    "print(sum(np.absolute(weights)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L<sub>2</sub> regularization represents model complexity as the sum of the squares of the feature weights (again multiplied by a constant, *lambda*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![L2 regularization](../nb-images/reg_formula2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same weights from above, the L<sub>2</sub> regularization term can be computed with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(np.square(weights))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use either of these regularization terms when fitting a model, the usual procedure is to add the regularization term to whatever loss function you want to use.  E.g., *total loss* = *loss* + *regularization term*.\n",
    "\n",
    "\n",
    "### 2.a. Lambda\n",
    "\n",
    "The regularization parameter in the above formulas, *lambda*, allows you to adjust the balance between minimizing the loss function and penalizing overly complex models.  Increasing *lambda* strengthens the regularization effect and will cause more of the model weights to be near zero, while decreasing *lambda* places more emphasis on reducing the loss function.  Thus, the value of *lambda* is very important during model fitting.  Too large a value for *lambda* and your model might be overly simple and prone to underfitting.  Too small of a value and your model will be more complex, but you run the risk of overfitting. The optimal value of lambda is data-dependent and will usually need to be estimated in some way.\n",
    "\n",
    "\n",
    "### 2.b. Practical differences between L<sub>1</sub> and L<sub>2</sub> regularization\n",
    "\n",
    "L<sub>1</sub> and L<sub>2</sub> both can help prevent overfitting.  From a practical standpoint, perhaps the most important difference between the two is that L<sub>1</sub> regularization can help with *feature selection*.  As a consequence of the mathematical properties of L<sub>1</sub> regularization, L<sub>1</sub> regularization can result in models where some of the feature weights are 0, effectively removing those features from the model.  In contrast, L<sub>2</sub> regularization can decrease model weights but not drive them to 0.  L<sub>1</sub> regularization can also be more robust and resistant to larger outliers in the data.  On the other hand, L<sub>2</sub> regularization results in a minimization problem with a unique solution, which is not always the case for L<sub>1</sub> regularization.  Which regularization method is best depends on the specifics of the data, the modeling problem, and the goals of the analysis.\n",
    " \n",
    "\n",
    "## 3. Practice example / demonstration\n",
    "\n",
    "Now, let's use scikit-learn to take a look at these ideas in actual practice.  We'll start by working with a dataset called `regularization.csv` that you can find in the `nb-datasets` folder.\n",
    "\n",
    "### Step 1: Import the required libraries and load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "\n",
    "rdata = pd.read_csv('../nb-datasets/regularization.csv')\n",
    "rdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Prepare the training and testing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the x and y values.\n",
    "x = rdata.drop(columns='response')\n",
    "y = rdata['response']\n",
    "\n",
    "# Split the train and test sets.  Use 25% of the data for testing.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Fit a standard linear regression model\n",
    "\n",
    "We'll give regular old non-regularized linear regression a try first.  As a reminder, we'll fit the model using the training data and then check how it performs on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Take a look at the model's R^2 score and training loss (mean squared error).\n",
    "print('Train R2:', model.score(x_train, y_train))\n",
    "train_loss = mse(y_train, model.predict(x_train))\n",
    "print('Train loss:', train_loss)\n",
    "\n",
    "# Let's also have a look at the model's coefficients.\n",
    "print('\\nCoefficients:', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Check the model's performance on the testing data\n",
    "\n",
    "Our model predicts the training responses extremely well (almost perfectly!).  Let's see how it does on the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test R2:', model.score(x_test, y_test))\n",
    "\n",
    "test_loss = mse(y_test, model.predict(x_test))\n",
    "print('Test loss:', test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh.  Predictions on \"new\" data are terrible!  This result indicates that our model is probably badly overfit to the training data.\n",
    "\n",
    "\n",
    "### Step 5: Try using L<sub>1</sub> regularization\n",
    "\n",
    "Let's see if we can use L<sub>1</sub> regularization to fix this.  In scikit-learn, the `Lasso` object provides linear regression with L<sub>1</sub> regularization.  We use it in much the same way as `LinearRegression`, but we need to provide a value for the parameter `alpha`, which corresponds with the parameter *lambda* discussed in the regularization formulas above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(alpha=20.0)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print('Train R2:', model.score(x_train, y_train))\n",
    "train_loss = mse(y_train, model.predict(x_train))\n",
    "print('Train loss:', train_loss)\n",
    "\n",
    "print('\\nTest R2:', model.score(x_test, y_test))\n",
    "test_loss = mse(y_test, model.predict(x_test))\n",
    "print('Test loss:', test_loss)\n",
    "\n",
    "print('\\nCoefficients:', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great success!  As is typical with regularization, the model's performance on the training data has decreased a little bit, but its performance on the testing data has massively improved.  The model's performance on \"new\" data is now competitive with its performance on the training data, which is about the best we can hope for.\n",
    "\n",
    "Note also that with a regularization parameter (`alpha`/*lambda* ) of 20.0, we've applied fairly aggressive regularization to our model fit, which has resulted in the L<sub>1</sub> regularization driving *nearly all* of the model's coefficients/feature weights to 0!  Only the coefficient for `feature_4` remains.  In other words, the L<sub>1</sub> regularization has drastically simplified our model by eliminating all features except for `feature_4`.\n",
    "\n",
    "Does that result make sense?  As it turns out, it does.  The dataset we've been using is an artificial dataset that I created for this exercise, so we know the true relationship of the features to the response.  As suggested by the regularized coefficient estimates, `feature_4` is the only feature that has any relationship with the response variable.  In reality, `response` is equal to `feature_4` * 1.2 plus a small amount of random noise.  *All* of the other features are entirely random and have no relationship to `response` at all.\n",
    "\n",
    "Although this is a contrived dataset, it is not as unrealistic as it might at first seem.  With complex real-world data, we have often have a large number of candidate features/predictor variables, and we don't know which, if any, of those features have any real relationship with the response variable.  In those situations, overfitting is a real concern, especially if the number of observations is relatively small, and regularization can help prevent us from drawing the wrong conclusions from our data.\n",
    "\n",
    "What about the value of the regularization parameter, `alpha`/*lambda*?  Here, I deliberately chose a value (20.0) that I knew would work well given the truth behind the dataset.  With real data, we never have that luxury, and more sophisticated methods (that we don't have time to present in this short course) are required to select an appropriate value of *lambda*.\n",
    "\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Try experimenting with the value of the regularization parameter in the code above.  How does changing the value of alpha affect the results?  When do you get results that are misleading or just plain wrong?\n",
    "\n",
    "\n",
    "### Step 6: Try using L<sub>2</sub> regularization\n",
    "\n",
    "We can also analyze these data using L<sub>2</sub> regularization.  To do that, we'll use a scikit-learn object called `Ridge` that implements linear regression with L<sub>2</sub> regularization.  The interface of `Ridge` is exactly the same as the interface of `Lasso`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(alpha=1.0)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print('Train R2:', model.score(x_train, y_train))\n",
    "train_loss = mse(y_train, model.predict(x_train))\n",
    "print('Train loss:', train_loss)\n",
    "\n",
    "print('\\nTest R2:', model.score(x_test, y_test))\n",
    "test_loss = mse(y_test, model.predict(x_test))\n",
    "print('Test loss:', test_loss)\n",
    "\n",
    "print('\\nCoefficients:', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This illustrates a key difference between L<sub>1</sub> regularization and L<sub>2</sub> regularization: L<sub>1</sub> regularization is able to drive feature weights/model coefficients to 0 and effectively remove features from a model, but L<sub>2</sub> regularization cannot, even when doing so would be appropriate.\n",
    "\n",
    "Consequently, for these data, L<sub>2</sub> regularization helps a bit with overfitting, but it clearly does not perform nearly as well as L<sub>1</sub> regularization.  That won't always be the case, though.  This dataset is an intentionally extreme examle that suits L<sub>1</sub> regularization especially well.  For datasets where all or most of the features really do have some connection to the response or where collinearity is a concern, L<sub>2</sub> regularization might be a better choice.\n",
    "\n",
    "\n",
    "## 4.  Practice example using real data\n",
    "\n",
    "As a final example, let's try using regularization on a real dataset.  We'll again use the iris dataset that you've already seen in previous lessons.  We might not have time for this example during the workshop, and if not, I encourage you to explore it on your own.\n",
    "\n",
    "### Steps 1 and 2: Load the data and split out training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idata = pd.read_csv('../nb-datasets/iris_dataset.csv')\n",
    "idata['species'] = idata['species'].astype('category')\n",
    "\n",
    "# Convert the categorical variable \"species\" to 1-hot encoding (AKA \"dummy variables\"),\n",
    "# but eliminate the first dummy variable because it is collinear with the other two\n",
    "# and does not provide any additional information.\n",
    "idata_enc = pd.get_dummies(idata, drop_first=True)\n",
    "\n",
    "# Separate the x and y values.\n",
    "x = idata_enc.drop(columns='petal_length')\n",
    "y = idata_enc['petal_length']\n",
    "\n",
    "# Split the train and test sets.\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25)\n",
    "\n",
    "# See what we have.\n",
    "idata_enc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps 3 and 4: Give standard linear regression a try\n",
    "\n",
    "We'll fit a standard linear regression model and check its performance on the testing data.  This will give us a baseline to compare to the regularization methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print('Train R2:', model.score(x_train, y_train))\n",
    "train_loss = mse(y_train, model.predict(x_train))\n",
    "print('Train loss:', train_loss)\n",
    "\n",
    "print('\\nTest R2:', model.score(x_test, y_test))\n",
    "test_loss = mse(y_test, model.predict(x_test))\n",
    "print('Test loss:', test_loss)\n",
    "\n",
    "print('Coefficients:', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: L<sub>1</sub> regularization\n",
    "\n",
    "The results above *strongly* suggest that overfitting really isn't much of a problem here.  We have a relatively small number of features (i.e., parameters we must estimate) compared to the number of observations, so that result makes sense.  Nevertheless, let's try fitting the model using L<sub>1</sub> regularization and see what happens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Lasso(alpha=0.01)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print('Train R2:', model.score(x_train, y_train))\n",
    "train_loss = mse(y_train, model.predict(x_train))\n",
    "print('Train loss:', train_loss)\n",
    "\n",
    "print('\\nTest R2:', model.score(x_test, y_test))\n",
    "test_loss = mse(y_test, model.predict(x_test))\n",
    "print('Test loss:', test_loss)\n",
    "\n",
    "print('\\nCoefficients:', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: L<sub>2</sub> regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(alpha=2.0)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print('Train R2:', model.score(x_train, y_train))\n",
    "train_loss = mse(y_train, model.predict(x_train))\n",
    "print('Train loss:', train_loss)\n",
    "\n",
    "print('\\nTest R2:', model.score(x_test, y_test))\n",
    "test_loss = mse(y_test, model.predict(x_test))\n",
    "print('Test loss:', test_loss)\n",
    "\n",
    "print('\\nCoefficients:', model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "Try experimenting with the value of `alpha`/*lambda* in the code above for both L<sub>1</sub> regularization and L<sub>2</sub> regularization.  As you do so, consider these questions:\n",
    "\n",
    "1. How does changing the value of the regularization parameter affect the coefficient weights and training/test performance?\n",
    "2. What values of the regularization parameter give you the best test accuracy?\n",
    "3. For these data, does L<sub>1</sub> or L<sub>2</sub> regularization perform better?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
